# å·¥ä½œæµæ•°æ®æ˜ å°„ç³»ç»Ÿè®¾è®¡

## ğŸ“‹ æ¦‚è¿°

æœ¬æ–‡æ¡£æè¿°äº†å·¥ä½œæµå¼•æ“ä¸­æ•°æ®æ˜ å°„ç³»ç»Ÿçš„æŠ€æœ¯è®¾è®¡ã€‚è¯¥ç³»ç»ŸåŸºäº[èŠ‚ç‚¹è§„èŒƒç³»ç»Ÿ](./node_spec.md)æ„å»ºï¼Œè§£å†³äº†å½“å‰è¿æ¥ç³»ç»Ÿåªèƒ½å®šä¹‰èŠ‚ç‚¹åˆ°èŠ‚ç‚¹çš„è¿æ¥ï¼Œä½†æ— æ³•å®šä¹‰**æ•°æ®å¦‚ä½•ä»ä¸Šæ¸¸èŠ‚ç‚¹çš„è¾“å‡ºç«¯å£è½¬æ¢åˆ°ä¸‹æ¸¸èŠ‚ç‚¹çš„è¾“å…¥ç«¯å£**çš„é—®é¢˜ã€‚

**å‰ç½®ä¾èµ–**: æœ¬ç³»ç»Ÿä¾èµ–äºèŠ‚ç‚¹è§„èŒƒç³»ç»Ÿä¸­å®šä¹‰çš„ç«¯å£è§„èŒƒ(`InputPortSpec`/`OutputPortSpec`)å’Œæ•°æ®æ ¼å¼è§„èŒƒ(`DataFormat`)ã€‚

## ğŸ¯ é—®é¢˜æè¿°

### å½“å‰è¿æ¥ç³»ç»Ÿçš„å±€é™æ€§

#### âŒ **ç¼ºå¤±çš„æ•°æ®è½¬æ¢èƒ½åŠ›**
```protobuf
// å½“å‰çš„è¿æ¥å®šä¹‰
message Connection {
  string node = 1;              // ç›®æ ‡èŠ‚ç‚¹å
  ConnectionType type = 2;      // è¿æ¥ç±»å‹
  int32 index = 3;             // ç«¯å£ç´¢å¼•
}
```

è¿™ç§å®šä¹‰åªèƒ½è¡¨è¾¾"è¿æ¥åˆ°å“ªä¸ªèŠ‚ç‚¹"ï¼Œä½†æ— æ³•æè¿°ï¼š
1. **æºç«¯å£é€‰æ‹©**: ä¸çŸ¥é“ä»æºèŠ‚ç‚¹çš„å“ªä¸ªè¾“å‡ºç«¯å£è·å–æ•°æ®
2. **æ•°æ®è½¬æ¢**: ä¸çŸ¥é“å¦‚ä½•å°†è¾“å‡ºæ•°æ®è½¬æ¢ä¸ºç›®æ ‡èŠ‚ç‚¹éœ€è¦çš„è¾“å…¥æ ¼å¼
3. **å­—æ®µæ˜ å°„**: ä¸çŸ¥é“æºæ•°æ®çš„å“ªäº›å­—æ®µå¯¹åº”ç›®æ ‡æ•°æ®çš„å“ªäº›å­—æ®µ
4. **æ•°æ®éªŒè¯**: ä¸çŸ¥é“è½¬æ¢åçš„æ•°æ®æ˜¯å¦ç¬¦åˆç›®æ ‡èŠ‚ç‚¹çš„è¾“å…¥è¦æ±‚

#### ğŸ¤” **å®é™…åœºæ™¯ä¸­çš„æ•°æ®è½¬æ¢éœ€æ±‚**

**åœºæ™¯**: RouterAgent â†’ TaskAnalyzer

```json
// RouterAgentçš„è¾“å‡º (source_port: "main")
{
  "route": "schedule_meeting",
  "confidence": 0.95,
  "reasoning": "ç”¨æˆ·æ˜ç¡®è¦æ±‚å®‰æ’ä¼šè®®",
  "metadata": {
    "timestamp": "2025-01-28T10:30:00Z",
    "user_id": "user_123"
  }
}

// TaskAnalyzeréœ€è¦çš„è¾“å…¥ (target_port: "main")
{
  "task_description": "schedule_meeting",     // æ¥è‡ª route å­—æ®µ
  "priority": "high",                         // åŸºäº confidence è®¡ç®—
  "context": {                               // é‡æ–°ç»„ç»‡çš„ä¸Šä¸‹æ–‡
    "confidence": 0.95,
    "reasoning": "ç”¨æˆ·æ˜ç¡®è¦æ±‚å®‰æ’ä¼šè®®",
    "user_id": "user_123",
    "processed_at": "2025-01-28T10:30:05Z"
  }
}
```

å½“å‰ç³»ç»Ÿæ— æ³•å¤„ç†è¿™ç§**ç»“æ„åŒ–çš„æ•°æ®è½¬æ¢**éœ€æ±‚ã€‚

## ğŸ—ï¸ è§£å†³æ–¹æ¡ˆæ¶æ„

### æ ¸å¿ƒè®¾è®¡åŸåˆ™

1. **å‘åå…¼å®¹**: ä¿ç•™ç°æœ‰è¿æ¥å®šä¹‰ï¼Œæ¸è¿›å¼å¢å¼º
2. **ç±»å‹å®‰å…¨**: åŸºäºèŠ‚ç‚¹è§„èŒƒè¿›è¡Œæ•°æ®éªŒè¯
3. **çµæ´»è½¬æ¢**: æ”¯æŒå¤šç§æ•°æ®æ˜ å°„ç­–ç•¥
4. **å¯è§†åŒ–å‹å¥½**: ä¾¿äºå‰ç«¯å±•ç¤ºæ•°æ®æµè½¬è¿‡ç¨‹
5. **æ€§èƒ½ä¼˜åŒ–**: é«˜æ•ˆçš„æ•°æ®è½¬æ¢æ‰§è¡Œ

### æ•´ä½“æ¶æ„å›¾

```mermaid
graph TB
    subgraph "èŠ‚ç‚¹Aè¾“å‡º"
        A1[è¾“å‡ºç«¯å£1: main]
        A2[è¾“å‡ºç«¯å£2: error]
    end

    subgraph "æ•°æ®æ˜ å°„å±‚"
        DM[DataMapping]
        DM --> FM[å­—æ®µæ˜ å°„]
        DM --> TM[æ¨¡æ¿è½¬æ¢]
        DM --> SM[è„šæœ¬è½¬æ¢]
        DM --> SV[é™æ€å€¼æ³¨å…¥]
    end

    subgraph "èŠ‚ç‚¹Bè¾“å…¥"
        B1[è¾“å…¥ç«¯å£1: main]
        B2[è¾“å…¥ç«¯å£2: config]
    end

    A1 --> DM
    DM --> B1

    style DM fill:#e1f5fe
    style FM fill:#f3e5f5
    style TM fill:#fff3e0
    style SM fill:#e8f5e8
    style SV fill:#fce4ec
```

## ğŸ”§ æ•°æ®ç»“æ„è®¾è®¡

### å¢å¼ºçš„è¿æ¥å®šä¹‰

**æ³¨æ„**: æ­¤è¿æ¥å®šä¹‰å·²æ•´åˆåˆ°[èŠ‚ç‚¹è§„èŒƒç³»ç»Ÿ](./node_spec.md)çš„Protocol Buffer schemaä¸­ã€‚

```protobuf
// å¢å¼ºçš„è¿æ¥å®šä¹‰ - æ¥è‡ªèŠ‚ç‚¹è§„èŒƒç³»ç»Ÿ
message Connection {
  string node = 1;                    // ç›®æ ‡èŠ‚ç‚¹å
  ConnectionType type = 2;            // è¿æ¥ç±»å‹
  int32 index = 3;                   // ç«¯å£ç´¢å¼•ï¼ˆå‘åå…¼å®¹ï¼‰

  // ç«¯å£çº§è¿æ¥ - åŸºäºNodeSpecä¸­çš„ç«¯å£å®šä¹‰
  string source_port = 4;            // æºç«¯å£åç§°ï¼Œå¿…é¡»å­˜åœ¨äºæºèŠ‚ç‚¹çš„OutputPortSpecä¸­
  string target_port = 5;            // ç›®æ ‡ç«¯å£åç§°ï¼Œå¿…é¡»å­˜åœ¨äºç›®æ ‡èŠ‚ç‚¹çš„InputPortSpecä¸­

  // æ•°æ®æ˜ å°„è§„åˆ™
  DataMapping data_mapping = 6;      // æ•°æ®è½¬æ¢è§„åˆ™
}

// æ•°æ®æ˜ å°„å®šä¹‰
message DataMapping {
  MappingType type = 1;                       // æ˜ å°„ç±»å‹
  repeated FieldMapping field_mappings = 2;  // å­—æ®µæ˜ å°„åˆ—è¡¨
  string transform_script = 3;               // è‡ªå®šä¹‰è½¬æ¢è„šæœ¬
  map<string, string> static_values = 4;     // é™æ€å€¼æ³¨å…¥
  string description = 5;                    // æ˜ å°„æè¿°
}

// æ˜ å°„ç±»å‹æšä¸¾
enum MappingType {
  DIRECT = 0;        // ç›´æ¥ä¼ é€’ï¼ˆé»˜è®¤è¡Œä¸ºï¼‰
  FIELD_MAPPING = 1; // å­—æ®µçº§æ˜ å°„
  TEMPLATE = 2;      // åŸºäºæ¨¡æ¿çš„è½¬æ¢
  TRANSFORM = 3;     // è‡ªå®šä¹‰è„šæœ¬è½¬æ¢
}

// å­—æ®µæ˜ å°„å®šä¹‰
message FieldMapping {
  string source_field = 1;          // æºå­—æ®µè·¯å¾„ï¼Œæ”¯æŒJSONPath
  string target_field = 2;          // ç›®æ ‡å­—æ®µè·¯å¾„
  FieldTransform transform = 3;     // å­—æ®µçº§è½¬æ¢è§„åˆ™
  bool required = 4;                // æ˜¯å¦å¿…éœ€å­—æ®µ
  string default_value = 5;         // é»˜è®¤å€¼
}

// å­—æ®µè½¬æ¢å®šä¹‰
message FieldTransform {
  TransformType type = 1;           // è½¬æ¢ç±»å‹
  string transform_value = 2;       // è½¬æ¢å‚æ•°
  map<string, string> options = 3;  // è½¬æ¢é€‰é¡¹
}

// å­—æ®µè½¬æ¢ç±»å‹
enum TransformType {
  NONE = 0;           // æ— è½¬æ¢
  STRING_FORMAT = 1;  // å­—ç¬¦ä¸²æ ¼å¼åŒ–ï¼šsprintfé£æ ¼
  JSON_PATH = 2;      // JSONPathè¡¨è¾¾å¼æå–
  REGEX = 3;          // æ­£åˆ™è¡¨è¾¾å¼è½¬æ¢
  FUNCTION = 4;       // å†…ç½®å‡½æ•°è°ƒç”¨
  CONDITION = 5;      // æ¡ä»¶åˆ¤æ–­è½¬æ¢
}
```

## ğŸ“ æ˜ å°„ç±»å‹è¯¦ç»†è¯´æ˜

### 1. ç›´æ¥ä¼ é€’ (DIRECT)

æœ€ç®€å•çš„æ˜ å°„æ–¹å¼ï¼Œç›´æ¥å°†æºæ•°æ®ä¼ é€’ç»™ç›®æ ‡èŠ‚ç‚¹ã€‚

```json
{
  "data_mapping": {
    "type": "DIRECT"
  }
}
```

**æ‰§è¡Œæ•ˆæœ**:
```javascript
// æºæ•°æ®
input = {"result": "success", "data": [1, 2, 3]}

// ç›®æ ‡æ•°æ®
output = {"result": "success", "data": [1, 2, 3]}  // å®Œå…¨ç›¸åŒ
```

### 2. å­—æ®µæ˜ å°„ (FIELD_MAPPING)

é€šè¿‡å­—æ®µè·¯å¾„è¿›è¡Œç²¾ç¡®çš„æ•°æ®æ˜ å°„å’Œè½¬æ¢ã€‚

```json
{
  "data_mapping": {
    "type": "FIELD_MAPPING",
    "field_mappings": [
      {
        "source_field": "route",
        "target_field": "task_description",
        "required": true
      },
      {
        "source_field": "confidence",
        "target_field": "context.confidence"
      },
      {
        "source_field": "confidence",
        "target_field": "priority",
        "transform": {
          "type": "CONDITION",
          "transform_value": "{{value}} > 0.8 ? 'high' : 'normal'"
        }
      },
      {
        "source_field": "metadata.user_id",
        "target_field": "context.user_id"
      }
    ],
    "static_values": {
      "context.processed_at": "{{current_time}}",
      "context.workflow_id": "{{workflow_id}}"
    }
  }
}
```

**æ‰§è¡Œæ•ˆæœ**:
```javascript
// æºæ•°æ®
input = {
  "route": "schedule_meeting",
  "confidence": 0.95,
  "reasoning": "ç”¨æˆ·è¦å®‰æ’ä¼šè®®",
  "metadata": {"user_id": "user_123"}
}

// ç›®æ ‡æ•°æ®
output = {
  "task_description": "schedule_meeting",
  "priority": "high",  // åŸºäºconfidence > 0.8è®¡ç®—
  "context": {
    "confidence": 0.95,
    "user_id": "user_123",
    "processed_at": "2025-01-28T10:30:05Z",
    "workflow_id": "wf_456"
  }
}
```

### 3. æ¨¡æ¿è½¬æ¢ (TEMPLATE)

ä½¿ç”¨ç±»ä¼¼Handlebarsçš„æ¨¡æ¿è¯­æ³•è¿›è¡Œæ•°æ®è½¬æ¢ã€‚

```json
{
  "data_mapping": {
    "type": "TEMPLATE",
    "transform_script": `{
      "task_description": "{{route}}",
      "priority": "{{confidence > 0.8 ? 'high' : 'normal'}}",
      "context": {
        "original_request": {
          "route": "{{route}}",
          "confidence": {{confidence}},
          "reasoning": "{{reasoning}}"
        },
        "processing_info": {
          "processed_at": "{{current_time}}",
          "workflow_id": "{{workflow_id}}",
          "node_id": "{{node_id}}"
        },
        "user_context": {{metadata}}
      }
    }`
  }
}
```

### 4. è„šæœ¬è½¬æ¢ (TRANSFORM)

ä½¿ç”¨JavaScriptæˆ–Pythonè„šæœ¬è¿›è¡Œå¤æ‚çš„æ•°æ®è½¬æ¢ã€‚

```json
{
  "data_mapping": {
    "type": "TRANSFORM",
    "transform_script": `
      function transform(input, context) {
        // å¤æ‚çš„ä¸šåŠ¡é€»è¾‘å¤„ç†
        const priority = calculatePriority(input.confidence, input.metadata);
        const enrichedContext = enrichContext(input, context);

        return {
          task_description: input.route,
          priority: priority,
          context: enrichedContext,
          validation: validateInput(input)
        };
      }

      function calculatePriority(confidence, metadata) {
        if (confidence > 0.9) return "urgent";
        if (confidence > 0.7) return "high";
        if (confidence > 0.5) return "normal";
        return "low";
      }

      function enrichContext(input, context) {
        return {
          ...input.metadata,
          confidence: input.confidence,
          reasoning: input.reasoning,
          processed_at: context.current_time,
          workflow_id: context.workflow_id
        };
      }

      function validateInput(input) {
        return {
          has_route: !!input.route,
          confidence_valid: input.confidence >= 0 && input.confidence <= 1,
          has_metadata: !!input.metadata
        };
      }
    `,
    "description": "æ™ºèƒ½è·¯ç”±ç»“æœè½¬æ¢ä¸ºä»»åŠ¡åˆ†æè¾“å…¥"
  }
}
```

## ğŸ” å­—æ®µè·¯å¾„è¡¨è¾¾å¼

### JSONPathæ”¯æŒ

æ”¯æŒæ ‡å‡†çš„JSONPathè¡¨è¾¾å¼è¿›è¡Œå­—æ®µæå–ï¼š

```javascript
// æºæ•°æ®
{
  "result": {
    "data": {
      "items": [
        {"name": "meeting1", "priority": "high"},
        {"name": "meeting2", "priority": "low"}
      ]
    }
  },
  "metadata": {
    "total": 2,
    "status": "success"
  }
}

// å­—æ®µè·¯å¾„ç¤ºä¾‹
"result.data.items[0].name"           // "meeting1"
"result.data.items[*].priority"       // ["high", "low"]
"metadata.total"                      // 2
"$.result.data.items[?(@.priority=='high')].name"  // ["meeting1"]
```

### å­—æ®µè½¬æ¢å‡½æ•°

æ”¯æŒå¸¸ç”¨çš„å†…ç½®è½¬æ¢å‡½æ•°ï¼š

```json
{
  "source_field": "timestamp",
  "target_field": "formatted_time",
  "transform": {
    "type": "FUNCTION",
    "transform_value": "date_format",
    "options": {
      "format": "YYYY-MM-DD HH:mm:ss",
      "timezone": "Asia/Shanghai"
    }
  }
}
```

**å†…ç½®å‡½æ•°åˆ—è¡¨**:
- `date_format`: æ—¥æœŸæ ¼å¼åŒ–
- `string_upper`: è½¬å¤§å†™
- `string_lower`: è½¬å°å†™
- `json_stringify`: JSONåºåˆ—åŒ–
- `json_parse`: JSONè§£æ
- `array_join`: æ•°ç»„è¿æ¥
- `array_length`: æ•°ç»„é•¿åº¦
- `math_round`: æ•°å€¼å››èˆäº”å…¥

## ğŸ”§ å®ç°æ¶æ„

### æ•°æ®æ˜ å°„å¤„ç†å™¨

```python
from shared.node_specs.registry import node_spec_registry
from shared.node_specs.validator import NodeSpecValidator

class DataMappingProcessor:
    """æ•°æ®æ˜ å°„å¤„ç†å™¨ï¼Œè´Ÿè´£æ‰§è¡Œå„ç§æ•°æ®è½¬æ¢"""

    def __init__(self):
        self.template_engine = TemplateEngine()
        self.script_engine = ScriptEngine()
        self.jsonpath_parser = JSONPathParser()
        self.function_registry = FunctionRegistry()
        self.node_registry = node_spec_registry  # å¼•ç”¨èŠ‚ç‚¹è§„èŒƒæ³¨å†Œå™¨
        self.validator = NodeSpecValidator()

    def transform_data(self,
                      source_data: Dict[str, Any],
                      mapping: DataMapping,
                      context: ExecutionContext,
                      source_node = None,
                      target_node = None,
                      source_port: str = "main",
                      target_port: str = "main") -> Dict[str, Any]:
        """æ ¹æ®æ˜ å°„è§„åˆ™è½¬æ¢æ•°æ®"""

        try:
            # éªŒè¯æºæ•°æ®æ ¼å¼ï¼ˆåŸºäºèŠ‚ç‚¹è§„èŒƒï¼‰
            if source_node:
                source_port_spec = self.node_registry.get_port_spec(
                    source_node.type, source_node.subtype, source_port, "output"
                )
                if source_port_spec:
                    validation_errors = self.validator.validate_port_data(source_port_spec, source_data)
                    if validation_errors:
                        raise DataMappingError(f"Source data validation failed: {', '.join(validation_errors)}")

            # æ‰§è¡Œæ•°æ®è½¬æ¢
            if mapping.type == MappingType.DIRECT:
                transformed_data = source_data
            elif mapping.type == MappingType.FIELD_MAPPING:
                transformed_data = self._apply_field_mappings(source_data, mapping, context)
            elif mapping.type == MappingType.TEMPLATE:
                transformed_data = self._apply_template_transform(source_data, mapping, context)
            elif mapping.type == MappingType.TRANSFORM:
                transformed_data = self._apply_script_transform(source_data, mapping, context)
            else:
                raise ValueError(f"Unsupported mapping type: {mapping.type}")

            # éªŒè¯è½¬æ¢åæ•°æ®æ ¼å¼ï¼ˆåŸºäºç›®æ ‡èŠ‚ç‚¹è§„èŒƒï¼‰
            if target_node:
                target_port_spec = self.node_registry.get_port_spec(
                    target_node.type, target_node.subtype, target_port, "input"
                )
                if target_port_spec:
                    validation_errors = self.validator.validate_port_data(target_port_spec, transformed_data)
                    if validation_errors:
                        raise DataMappingError(f"Target data validation failed: {', '.join(validation_errors)}")

            return transformed_data

        except Exception as e:
            self._log_mapping_error(mapping, source_data, e)
            raise DataMappingError(f"Data mapping failed: {str(e)}")

    def _apply_field_mappings(self,
                             source_data: Dict[str, Any],
                             mapping: DataMapping,
                             context: ExecutionContext) -> Dict[str, Any]:
        """åº”ç”¨å­—æ®µæ˜ å°„"""
        result = {}

        # å¤„ç†å­—æ®µæ˜ å°„
        for field_mapping in mapping.field_mappings:
            try:
                # æå–æºå­—æ®µå€¼
                source_value = self._extract_field_value(
                    source_data,
                    field_mapping.source_field
                )

                # æ£€æŸ¥å¿…éœ€å­—æ®µ
                if field_mapping.required and source_value is None:
                    if field_mapping.default_value:
                        source_value = field_mapping.default_value
                    else:
                        raise ValueError(f"Required field missing: {field_mapping.source_field}")

                # åº”ç”¨å­—æ®µçº§è½¬æ¢
                if field_mapping.transform:
                    source_value = self._apply_field_transform(
                        source_value,
                        field_mapping.transform,
                        context
                    )

                # è®¾ç½®ç›®æ ‡å­—æ®µå€¼
                self._set_field_value(result, field_mapping.target_field, source_value)

            except Exception as e:
                self._log_field_mapping_error(field_mapping, source_data, e)
                if field_mapping.required:
                    raise

        # å¤„ç†é™æ€å€¼æ³¨å…¥
        for key, value_template in mapping.static_values.items():
            resolved_value = self._resolve_template_value(value_template, context)
            self._set_field_value(result, key, resolved_value)

        return result

    def _extract_field_value(self, data: Dict[str, Any], field_path: str) -> Any:
        """ä½¿ç”¨JSONPathæå–å­—æ®µå€¼"""
        try:
            if field_path.startswith('$'):
                # ä½¿ç”¨JSONPathè¡¨è¾¾å¼
                return self.jsonpath_parser.extract(data, field_path)
            else:
                # ä½¿ç”¨ç®€å•çš„ç‚¹åˆ†å‰²è·¯å¾„
                return self._simple_field_access(data, field_path)
        except Exception as e:
            self.logger.warning(f"Field extraction failed for path '{field_path}': {e}")
            return None

    def _apply_field_transform(self,
                              value: Any,
                              transform: FieldTransform,
                              context: ExecutionContext) -> Any:
        """åº”ç”¨å­—æ®µçº§è½¬æ¢"""

        if transform.type == TransformType.NONE:
            return value

        elif transform.type == TransformType.STRING_FORMAT:
            return transform.transform_value.format(value=value)

        elif transform.type == TransformType.FUNCTION:
            func = self.function_registry.get_function(transform.transform_value)
            return func(value, **transform.options)

        elif transform.type == TransformType.CONDITION:
            return self._evaluate_condition(value, transform.transform_value, context)

        elif transform.type == TransformType.REGEX:
            import re
            pattern = transform.transform_value
            replacement = transform.options.get('replacement', '')
            return re.sub(pattern, replacement, str(value))

        else:
            raise ValueError(f"Unsupported transform type: {transform.type}")

@dataclass
class ExecutionContext:
    """æ‰§è¡Œä¸Šä¸‹æ–‡ï¼Œæä¾›ç¯å¢ƒå˜é‡å’Œè¿è¡Œæ—¶ä¿¡æ¯"""
    workflow_id: str
    execution_id: str
    node_id: str
    current_time: str
    user_id: Optional[str] = None
    session_id: Optional[str] = None
    environment: str = "production"

    def to_dict(self) -> Dict[str, Any]:
        return {
            "workflow_id": self.workflow_id,
            "execution_id": self.execution_id,
            "node_id": self.node_id,
            "current_time": self.current_time,
            "user_id": self.user_id,
            "session_id": self.session_id,
            "environment": self.environment
        }
```

### è¿æ¥æ‰§è¡Œå¼•æ“é›†æˆ

```python
class ConnectionExecutor:
    """è¿æ¥æ‰§è¡Œå™¨ï¼Œå¤„ç†èŠ‚ç‚¹é—´çš„æ•°æ®ä¼ é€’"""

    def __init__(self):
        self.data_mapper = DataMappingProcessor()
        self.node_registry = node_spec_registry  # ä½¿ç”¨ç»Ÿä¸€çš„èŠ‚ç‚¹è§„èŒƒæ³¨å†Œå™¨
        self.logger = logging.getLogger(__name__)

    def execute_connection(self,
                          source_node_result: NodeExecutionResult,
                          connection: Connection,
                          target_node: Node,
                          context: ExecutionContext) -> Dict[str, Any]:
        """æ‰§è¡Œè¿æ¥ï¼ŒåŒ…å«æ•°æ®æ˜ å°„å’ŒéªŒè¯"""

        # 1. è·å–æºç«¯å£æ•°æ®
        source_data = self._get_port_data(
            source_node_result,
            connection.source_port or "main"
        )

        if source_data is None:
            self.logger.warning(f"No data from source port: {connection.source_port}")
            return {}

        # 2. åº”ç”¨æ•°æ®æ˜ å°„ï¼ˆåŒ…å«èŠ‚ç‚¹è§„èŒƒéªŒè¯ï¼‰
        if connection.data_mapping:
            try:
                mapped_data = self.data_mapper.transform_data(
                    source_data,
                    connection.data_mapping,
                    context,
                    source_node=source_node_result.node,  # ä¼ é€’èŠ‚ç‚¹ä¿¡æ¯ç”¨äºéªŒè¯
                    target_node=target_node,
                    source_port=connection.source_port or "main",
                    target_port=connection.target_port or "main"
                )
                self.logger.debug(f"Data mapping applied: {connection.data_mapping.type}")
            except Exception as e:
                self.logger.error(f"Data mapping failed: {e}")
                raise ConnectionExecutionError(f"Data mapping failed: {str(e)}")
        else:
            # é»˜è®¤ç›´æ¥ä¼ é€’ï¼Œä½†ä»éœ€éªŒè¯æ ¼å¼å…¼å®¹æ€§
            try:
                mapped_data = self.data_mapper.transform_data(
                    source_data,
                    DataMapping(type=MappingType.DIRECT),  # åˆ›å»ºé»˜è®¤ç›´æ¥æ˜ å°„
                    context,
                    source_node=source_node_result.node,
                    target_node=target_node,
                    source_port=connection.source_port or "main",
                    target_port=connection.target_port or "main"
                )
            except Exception as e:
                self.logger.error(f"Direct data mapping validation failed: {e}")
                raise ConnectionExecutionError(f"Direct data mapping validation failed: {str(e)}")

        # 4. è®°å½•æ•°æ®æµè½¬æ—¥å¿—
        self._log_data_flow(source_node_result.node_id, target_node.id, connection, mapped_data)

        return mapped_data

    def _get_port_data(self,
                      node_result: NodeExecutionResult,
                      port_name: str) -> Optional[Dict[str, Any]]:
        """ä»èŠ‚ç‚¹æ‰§è¡Œç»“æœä¸­è·å–æŒ‡å®šç«¯å£çš„æ•°æ®"""

        if port_name == "main":
            return node_result.output_data

        # æ”¯æŒå¤šç«¯å£è¾“å‡º
        if hasattr(node_result, 'port_outputs') and node_result.port_outputs:
            return node_result.port_outputs.get(port_name)

        return None

    def _log_data_flow(self,
                      source_node_id: str,
                      target_node_id: str,
                      connection: Connection,
                      data: Dict[str, Any]):
        """è®°å½•æ•°æ®æµè½¬æ—¥å¿—ï¼Œç”¨äºè°ƒè¯•å’Œç›‘æ§"""
        self.logger.info(f"Data flow: {source_node_id}[{connection.source_port}] "
                        f"-> {target_node_id}[{connection.target_port}], "
                        f"mapping: {connection.data_mapping.type if connection.data_mapping else 'DIRECT'}")
```

### æ•°æ®éªŒè¯é›†æˆ

**æ³¨æ„**: æ•°æ®éªŒè¯åŠŸèƒ½å·²æ•´åˆåˆ°[èŠ‚ç‚¹è§„èŒƒç³»ç»Ÿ](./node_spec.md)çš„`NodeSpecValidator`ä¸­ã€‚æ•°æ®æ˜ å°„ç³»ç»Ÿç›´æ¥ä½¿ç”¨èŠ‚ç‚¹è§„èŒƒè¿›è¡ŒéªŒè¯ï¼Œæ— éœ€å•ç‹¬çš„éªŒè¯å™¨ã€‚

```python
# æ•°æ®æ˜ å°„ç³»ç»Ÿä½¿ç”¨èŠ‚ç‚¹è§„èŒƒç³»ç»Ÿçš„éªŒè¯åŠŸèƒ½
from shared.node_specs.validator import NodeSpecValidator
from shared.node_specs.registry import node_spec_registry

class DataMappingValidator:
    """æ•°æ®æ˜ å°„éªŒè¯å™¨ï¼ŒåŸºäºèŠ‚ç‚¹è§„èŒƒéªŒè¯æ•°æ®æ˜ å°„é…ç½®"""

    def __init__(self):
        self.spec_registry = node_spec_registry
        self.node_validator = NodeSpecValidator()

    def validate_mapping_configuration(self,
                                     source_node,
                                     target_node,
                                     connection: Connection) -> List[str]:
        """éªŒè¯æ•°æ®æ˜ å°„é…ç½®çš„åˆç†æ€§"""
        errors = []

        # 1. éªŒè¯ç«¯å£å­˜åœ¨æ€§å’Œå…¼å®¹æ€§
        port_errors = self.spec_registry.validate_connection(
            source_node, connection.source_port or "main",
            target_node, connection.target_port or "main"
        )
        errors.extend(port_errors)

        # 2. éªŒè¯æ•°æ®æ˜ å°„è§„åˆ™
        if connection.data_mapping:
            mapping_errors = self._validate_data_mapping_rules(
                source_node, target_node, connection.data_mapping
            )
            errors.extend(mapping_errors)

        return errors

    def _validate_data_mapping_rules(self, source_node, target_node, mapping) -> List[str]:
        """éªŒè¯æ•°æ®æ˜ å°„è§„åˆ™çš„è¯­æ³•å’Œé€»è¾‘"""
        errors = []

        if mapping.type == "FIELD_MAPPING":
            # éªŒè¯å­—æ®µæ˜ å°„é…ç½®
            for field_mapping in mapping.field_mappings:
                if not field_mapping.source_field:
                    errors.append("Field mapping missing source_field")
                if not field_mapping.target_field:
                    errors.append("Field mapping missing target_field")

        elif mapping.type == "TEMPLATE":
            # éªŒè¯æ¨¡æ¿è¯­æ³•
            if not mapping.transform_script:
                errors.append("Template mapping missing transform_script")

        elif mapping.type == "TRANSFORM":
            # éªŒè¯è½¬æ¢è„šæœ¬è¯­æ³•
            if not mapping.transform_script:
                errors.append("Transform mapping missing transform_script")
            else:
                # è¿™é‡Œå¯ä»¥æ·»åŠ JavaScript/Pythonè¯­æ³•æ£€æŸ¥
                pass

        return errors
```

## ğŸŒŸ ä½¿ç”¨ç¤ºä¾‹

### å®Œæ•´çš„å·¥ä½œæµé…ç½®ç¤ºä¾‹

```json
{
  "id": "customer-service-workflow",
  "name": "æ™ºèƒ½å®¢æœå¤„ç†æµç¨‹",
  "nodes": [
    {
      "id": "router",
      "name": "Router Agent",
      "type": "AI_AGENT_NODE",
      "subtype": "ROUTER_AGENT"
    },
    {
      "id": "task_analyzer",
      "name": "Task Analyzer",
      "type": "AI_AGENT_NODE",
      "subtype": "TASK_ANALYZER"
    },
    {
      "id": "priority_filter",
      "name": "Priority Filter",
      "type": "FLOW_NODE",
      "subtype": "IF"
    }
  ],
  "connections": {
    "connections": {
      "router": {
        "main": {
          "connections": [
            {
              "node": "task_analyzer",
              "source_port": "main",
              "target_port": "main",
              "data_mapping": {
                "type": "FIELD_MAPPING",
                "description": "è·¯ç”±ç»“æœè½¬æ¢ä¸ºä»»åŠ¡åˆ†æè¾“å…¥",
                "field_mappings": [
                  {
                    "source_field": "route",
                    "target_field": "task_description",
                    "required": true
                  },
                  {
                    "source_field": "confidence",
                    "target_field": "priority",
                    "transform": {
                      "type": "CONDITION",
                      "transform_value": "{{value}} > 0.8 ? 'high' : ({{value}} > 0.5 ? 'normal' : 'low')"
                    }
                  },
                  {
                    "source_field": "reasoning",
                    "target_field": "context.reasoning"
                  },
                  {
                    "source_field": "metadata.user_id",
                    "target_field": "context.user_id"
                  }
                ],
                "static_values": {
                  "context.source": "router_agent",
                  "context.processed_at": "{{current_time}}",
                  "context.workflow_id": "{{workflow_id}}"
                }
              }
            }
          ]
        }
      },
      "task_analyzer": {
        "main": {
          "connections": [
            {
              "node": "priority_filter",
              "source_port": "main",
              "target_port": "main",
              "data_mapping": {
                "type": "TEMPLATE",
                "description": "ä»»åŠ¡åˆ†æç»“æœè½¬æ¢ä¸ºä¼˜å…ˆçº§è¿‡æ»¤è¾“å…¥",
                "transform_script": `{
                  "condition_data": {
                    "priority": "{{priority}}",
                    "complexity": {{complexity}},
                    "urgency": "{{urgency}}"
                  },
                  "original_task": {
                    "description": "{{task_description}}",
                    "subtasks": {{subtasks}},
                    "estimated_time": {{estimated_time}}
                  },
                  "metadata": {
                    "analyzed_at": "{{current_time}}",
                    "analyzer_confidence": {{confidence}},
                    "workflow_context": {{context}}
                  }
                }`
              }
            }
          ]
        }
      }
    }
  }
}
```

### å¤æ‚è„šæœ¬è½¬æ¢ç¤ºä¾‹

```json
{
  "data_mapping": {
    "type": "TRANSFORM",
    "description": "å®¢æˆ·æœåŠ¡è¯·æ±‚æ™ºèƒ½å¤„ç†è½¬æ¢",
    "transform_script": `
      function transform(input, context) {
        // 1. è§£æå®¢æˆ·è¯·æ±‚
        const request = parseCustomerRequest(input);

        // 2. è®¡ç®—å¤„ç†ä¼˜å…ˆçº§
        const priority = calculateServicePriority(request, context);

        // 3. ç”Ÿæˆå¤„ç†å»ºè®®
        const suggestions = generateServiceSuggestions(request, priority);

        // 4. æ„å»ºè¾“å‡ºæ•°æ®
        return {
          customer_info: {
            id: request.customer_id,
            tier: request.customer_tier,
            history: request.interaction_history
          },
          request_analysis: {
            category: request.category,
            urgency: priority.urgency,
            complexity: priority.complexity,
            estimated_resolution_time: suggestions.estimated_time
          },
          recommended_actions: suggestions.actions,
          routing_decision: {
            department: suggestions.department,
            agent_type: suggestions.agent_type,
            escalation_required: priority.urgency === 'critical'
          },
          context: {
            analyzed_at: context.current_time,
            workflow_id: context.workflow_id,
            confidence_score: request.confidence || 0.8
          }
        };
      }

      function parseCustomerRequest(input) {
        return {
          customer_id: input.metadata?.customer_id || 'unknown',
          customer_tier: input.metadata?.tier || 'standard',
          category: classifyRequest(input.route),
          interaction_history: input.metadata?.history || [],
          confidence: input.confidence
        };
      }

      function calculateServicePriority(request, context) {
        let urgency = 'normal';
        let complexity = 'medium';

        // åŸºäºå®¢æˆ·ç­‰çº§è°ƒæ•´ä¼˜å…ˆçº§
        if (request.customer_tier === 'premium') {
          urgency = 'high';
        }

        // åŸºäºé—®é¢˜ç±»åˆ«è°ƒæ•´å¤æ‚åº¦
        if (['technical_issue', 'billing_dispute'].includes(request.category)) {
          complexity = 'high';
        }

        // åŸºäºå†å²äº¤äº’è°ƒæ•´
        if (request.interaction_history.length > 3) {
          urgency = 'critical';
        }

        return { urgency, complexity };
      }

      function generateServiceSuggestions(request, priority) {
        const suggestions = {
          actions: [],
          department: 'general',
          agent_type: 'human',
          estimated_time: 30
        };

        // æ ¹æ®ç±»åˆ«ç”Ÿæˆå»ºè®®
        switch (request.category) {
          case 'technical_issue':
            suggestions.department = 'technical';
            suggestions.actions = ['diagnose_issue', 'provide_solution', 'follow_up'];
            suggestions.estimated_time = 45;
            break;

          case 'billing_inquiry':
            suggestions.department = 'billing';
            suggestions.actions = ['verify_account', 'explain_charges', 'process_adjustment'];
            suggestions.estimated_time = 20;
            break;

          case 'general_inquiry':
            suggestions.agent_type = 'ai';
            suggestions.actions = ['provide_information', 'offer_resources'];
            suggestions.estimated_time = 15;
            break;
        }

        // é«˜ä¼˜å…ˆçº§ä»»åŠ¡è°ƒæ•´
        if (priority.urgency === 'critical') {
          suggestions.agent_type = 'senior_human';
          suggestions.estimated_time *= 1.5;
        }

        return suggestions;
      }

      function classifyRequest(route) {
        const categoryMap = {
          'technical_support': 'technical_issue',
          'billing_question': 'billing_inquiry',
          'general_help': 'general_inquiry',
          'complaint': 'complaint_handling'
        };

        return categoryMap[route] || 'general_inquiry';
      }
    `
  }
}
```

## ğŸ§ª æµ‹è¯•ç­–ç•¥

### å•å…ƒæµ‹è¯•

**æ³¨æ„**: æµ‹è¯•éœ€è¦ä¾èµ–èŠ‚ç‚¹è§„èŒƒç³»ç»Ÿæä¾›çš„æµ‹è¯•èŠ‚ç‚¹è§„èŒƒã€‚

```python
import pytest
from workflow_engine.data_mapping import DataMappingProcessor, ExecutionContext
from shared.node_specs.registry import node_spec_registry
from shared.node_specs.definitions.test_specs import create_test_node_specs  # æµ‹è¯•ä¸“ç”¨è§„èŒƒ

class TestDataMappingProcessor:

    def setup_method(self):
        # è®¾ç½®æµ‹è¯•èŠ‚ç‚¹è§„èŒƒ
        create_test_node_specs()

        self.processor = DataMappingProcessor()
        self.context = ExecutionContext(
            workflow_id="test_wf_123",
            execution_id="test_exec_456",
            node_id="test_node",
            current_time="2025-01-28T10:30:00Z"
        )

        # åˆ›å»ºæµ‹è¯•èŠ‚ç‚¹å¯¹è±¡
        self.source_node = self._create_test_node("AI_AGENT_NODE", "ROUTER_AGENT")
        self.target_node = self._create_test_node("AI_AGENT_NODE", "TASK_ANALYZER")

    def _create_test_node(self, node_type: str, subtype: str):
        """åˆ›å»ºæµ‹è¯•èŠ‚ç‚¹å¯¹è±¡"""
        return type('Node', (), {
            'type': node_type,
            'subtype': subtype,
            'id': f'test_{subtype.lower()}'
        })()

    def test_direct_mapping(self):
        """æµ‹è¯•ç›´æ¥æ˜ å°„"""
        source_data = {"result": "success", "data": [1, 2, 3]}
        mapping = DataMapping(type=MappingType.DIRECT)

        result = self.processor.transform_data(
            source_data, mapping, self.context,
            source_node=self.source_node, target_node=self.target_node
        )

        assert result == source_data

    def test_field_mapping_basic(self):
        """æµ‹è¯•åŸºç¡€å­—æ®µæ˜ å°„"""
        source_data = {
            "route": "schedule_meeting",
            "confidence": 0.95,
            "metadata": {"user_id": "user_123"}
        }

        mapping = DataMapping(
            type=MappingType.FIELD_MAPPING,
            field_mappings=[
                FieldMapping(
                    source_field="route",
                    target_field="task_description",
                    required=True
                ),
                FieldMapping(
                    source_field="metadata.user_id",
                    target_field="context.user_id"
                )
            ],
            static_values={
                "context.workflow_id": "{{workflow_id}}"
            }
        )

        result = self.processor.transform_data(
            source_data, mapping, self.context,
            source_node=self.source_node, target_node=self.target_node
        )

        expected = {
            "task_description": "schedule_meeting",
            "context": {
                "user_id": "user_123",
                "workflow_id": "test_wf_123"
            }
        }

        assert result == expected

    def test_field_mapping_with_transform(self):
        """æµ‹è¯•å¸¦è½¬æ¢çš„å­—æ®µæ˜ å°„"""
        source_data = {"confidence": 0.85}

        mapping = DataMapping(
            type=MappingType.FIELD_MAPPING,
            field_mappings=[
                FieldMapping(
                    source_field="confidence",
                    target_field="priority",
                    transform=FieldTransform(
                        type=TransformType.CONDITION,
                        transform_value="{{value}} > 0.8 ? 'high' : 'normal'"
                    )
                )
            ]
        )

        result = self.processor.transform_data(source_data, mapping, self.context)

        assert result == {"priority": "high"}

    def test_template_mapping(self):
        """æµ‹è¯•æ¨¡æ¿æ˜ å°„"""
        source_data = {
            "route": "schedule_meeting",
            "confidence": 0.95
        }

        mapping = DataMapping(
            type=MappingType.TEMPLATE,
            transform_script='''{
                "task": "{{route}}",
                "priority": "{{confidence > 0.8 ? 'high' : 'normal'}}",
                "processed_at": "{{current_time}}"
            }'''
        )

        result = self.processor.transform_data(source_data, mapping, self.context)

        expected = {
            "task": "schedule_meeting",
            "priority": "high",
            "processed_at": "2025-01-28T10:30:00Z"
        }

        assert result == expected

    def test_missing_required_field(self):
        """æµ‹è¯•ç¼ºå°‘å¿…éœ€å­—æ®µçš„é”™è¯¯å¤„ç†"""
        source_data = {"confidence": 0.95}  # ç¼ºå°‘routeå­—æ®µ

        mapping = DataMapping(
            type=MappingType.FIELD_MAPPING,
            field_mappings=[
                FieldMapping(
                    source_field="route",
                    target_field="task_description",
                    required=True
                )
            ]
        )

        with pytest.raises(ValueError, match="Required field missing: route"):
            self.processor.transform_data(source_data, mapping, self.context)

    def test_jsonpath_extraction(self):
        """æµ‹è¯•JSONPathå­—æ®µæå–"""
        source_data = {
            "result": {
                "data": {
                    "items": [
                        {"name": "item1", "value": 100},
                        {"name": "item2", "value": 200}
                    ]
                }
            }
        }

        mapping = DataMapping(
            type=MappingType.FIELD_MAPPING,
            field_mappings=[
                FieldMapping(
                    source_field="result.data.items[0].name",
                    target_field="first_item_name"
                ),
                FieldMapping(
                    source_field="result.data.items[*].value",
                    target_field="all_values"
                )
            ]
        )

        result = self.processor.transform_data(source_data, mapping, self.context)

        expected = {
            "first_item_name": "item1",
            "all_values": [100, 200]
        }

        assert result == expected
```

### é›†æˆæµ‹è¯•

```python
class TestConnectionExecution:

    def test_end_to_end_data_flow(self):
        """æµ‹è¯•ç«¯åˆ°ç«¯çš„æ•°æ®æµè½¬"""

        # æ¨¡æ‹ŸæºèŠ‚ç‚¹æ‰§è¡Œç»“æœ
        source_result = NodeExecutionResult(
            status=ExecutionStatus.SUCCESS,
            output_data={
                "route": "technical_support",
                "confidence": 0.92,
                "reasoning": "ç”¨æˆ·æŠ¥å‘Šç³»ç»Ÿbug",
                "metadata": {
                    "user_id": "user_456",
                    "timestamp": "2025-01-28T10:25:00Z"
                }
            }
        )

        # å®šä¹‰è¿æ¥é…ç½®
        connection = Connection(
            node="task_analyzer",
            source_port="main",
            target_port="main",
            data_mapping=DataMapping(
                type=MappingType.FIELD_MAPPING,
                field_mappings=[
                    FieldMapping(
                        source_field="route",
                        target_field="task_description",
                        required=True
                    ),
                    FieldMapping(
                        source_field="confidence",
                        target_field="priority",
                        transform=FieldTransform(
                            type=TransformType.CONDITION,
                            transform_value="{{value}} > 0.9 ? 'urgent' : 'normal'"
                        )
                    ),
                    FieldMapping(
                        source_field="metadata.user_id",
                        target_field="context.user_id"
                    )
                ],
                static_values={
                    "context.processed_at": "{{current_time}}",
                    "context.source": "router_agent"
                }
            )
        )

        # æ‰§è¡Œè¿æ¥
        executor = ConnectionExecutor()
        context = ExecutionContext(
            workflow_id="wf_789",
            execution_id="exec_012",
            node_id="router",
            current_time="2025-01-28T10:30:00Z"
        )

        result = executor.execute_connection(
            source_result,
            connection,
            target_node,
            context
        )

        expected = {
            "task_description": "technical_support",
            "priority": "urgent",  # confidence > 0.9
            "context": {
                "user_id": "user_456",
                "processed_at": "2025-01-28T10:30:00Z",
                "source": "router_agent"
            }
        }

        assert result == expected
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

### ç¼“å­˜ç­–ç•¥

```python
class CachedDataMappingProcessor(DataMappingProcessor):
    """å¸¦ç¼“å­˜åŠŸèƒ½çš„æ•°æ®æ˜ å°„å¤„ç†å™¨"""

    def __init__(self):
        super().__init__()
        self.template_cache = LRUCache(maxsize=1000)
        self.script_cache = LRUCache(maxsize=500)
        self.jsonpath_cache = LRUCache(maxsize=2000)

    def _apply_template_transform(self, source_data, mapping, context):
        """ç¼“å­˜æ¨¡æ¿ç¼–è¯‘ç»“æœ"""
        cache_key = hash(mapping.transform_script)

        if cache_key not in self.template_cache:
            compiled_template = self.template_engine.compile(mapping.transform_script)
            self.template_cache[cache_key] = compiled_template
        else:
            compiled_template = self.template_cache[cache_key]

        return compiled_template.render(source=source_data, context=context.to_dict())

    def _extract_field_value(self, data, field_path):
        """ç¼“å­˜JSONPathç¼–è¯‘ç»“æœ"""
        if field_path.startswith('$'):
            cache_key = field_path

            if cache_key not in self.jsonpath_cache:
                compiled_path = self.jsonpath_parser.compile(field_path)
                self.jsonpath_cache[cache_key] = compiled_path
            else:
                compiled_path = self.jsonpath_cache[cache_key]

            return compiled_path.extract(data)

        return super()._extract_field_value(data, field_path)
```

### å¹¶å‘å¤„ç†

```python
import asyncio
from concurrent.futures import ThreadPoolExecutor

class AsyncDataMappingProcessor:
    """å¼‚æ­¥æ•°æ®æ˜ å°„å¤„ç†å™¨ï¼Œæ”¯æŒå¹¶å‘è½¬æ¢"""

    def __init__(self, max_workers=4):
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.sync_processor = DataMappingProcessor()

    async def transform_data_async(self, source_data, mapping, context):
        """å¼‚æ­¥æ‰§è¡Œæ•°æ®è½¬æ¢"""
        loop = asyncio.get_event_loop()

        return await loop.run_in_executor(
            self.executor,
            self.sync_processor.transform_data,
            source_data,
            mapping,
            context
        )

    async def transform_multiple_connections(self, connections_data):
        """å¹¶å‘å¤„ç†å¤šä¸ªè¿æ¥çš„æ•°æ®è½¬æ¢"""
        tasks = []

        for source_data, mapping, context in connections_data:
            task = self.transform_data_async(source_data, mapping, context)
            tasks.append(task)

        results = await asyncio.gather(*tasks, return_exceptions=True)

        # å¤„ç†å¼‚å¸¸ç»“æœ
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                self.logger.error(f"Connection {i} mapping failed: {result}")
                processed_results.append({})
            else:
                processed_results.append(result)

        return processed_results
```

## ğŸ¯ è¿ç§»ç­–ç•¥

### å‘åå…¼å®¹æ€§

1. **ä¿ç•™ç°æœ‰å­—æ®µ**: `index` å­—æ®µç»§ç»­ä¿ç•™ï¼Œç”¨äºå‘åå…¼å®¹
2. **é»˜è®¤è¡Œä¸º**: æ²¡æœ‰ `data_mapping` æ—¶é»˜è®¤ä½¿ç”¨ `DIRECT` æ¨¡å¼
3. **æ¸è¿›å¼å‡çº§**: æ–°å·¥ä½œæµä½¿ç”¨æ–°æ ¼å¼ï¼Œè€å·¥ä½œæµç»§ç»­å·¥ä½œ

### è¿ç§»å·¥å…·

```python
class WorkflowMigrationTool:
    """å·¥ä½œæµè¿ç§»å·¥å…·ï¼Œå°†æ—§æ ¼å¼è½¬æ¢ä¸ºæ–°æ ¼å¼"""

    def migrate_workflow(self, old_workflow: Dict[str, Any]) -> Dict[str, Any]:
        """è¿ç§»å·¥ä½œæµå®šä¹‰"""
        new_workflow = old_workflow.copy()

        if 'connections' in new_workflow:
            new_workflow['connections'] = self._migrate_connections(
                old_workflow['connections']
            )

        return new_workflow

    def _migrate_connections(self, old_connections: Dict[str, Any]) -> Dict[str, Any]:
        """è¿ç§»è¿æ¥å®šä¹‰"""
        new_connections = {"connections": {}}

        for node_name, node_conns in old_connections.get('connections', {}).items():
            new_node_conns = {"connection_types": {}}

            for conn_type, conn_array in node_conns.get('connection_types', {}).items():
                new_conn_array = {"connections": []}

                for connection in conn_array.get('connections', []):
                    new_connection = {
                        "node": connection.get("node"),
                        "type": connection.get("type", "MAIN"),
                        "index": connection.get("index", 0),
                        # æ–°å¢å­—æ®µï¼Œä½¿ç”¨é»˜è®¤å€¼
                        "source_port": "main",
                        "target_port": "main"
                        # data_mapping é»˜è®¤ä¸ºç©ºï¼Œä½¿ç”¨DIRECTæ¨¡å¼
                    }
                    new_conn_array["connections"].append(new_connection)

                new_node_conns["connection_types"][conn_type] = new_conn_array

            new_connections["connections"][node_name] = new_node_conns

        return new_connections
```

## ğŸ” ç›‘æ§å’Œè°ƒè¯•

### æ•°æ®æµè½¬ç›‘æ§

```python
class DataFlowMonitor:
    """æ•°æ®æµè½¬ç›‘æ§å™¨"""

    def __init__(self):
        self.metrics_collector = MetricsCollector()
        self.trace_logger = TraceLogger()

    def record_data_mapping(self,
                           source_node: str,
                           target_node: str,
                           mapping_type: str,
                           execution_time: float,
                           data_size: int,
                           success: bool):
        """è®°å½•æ•°æ®æ˜ å°„æŒ‡æ ‡"""

        self.metrics_collector.increment('data_mapping_total', tags={
            'source_node': source_node,
            'target_node': target_node,
            'mapping_type': mapping_type,
            'success': str(success)
        })

        self.metrics_collector.histogram('data_mapping_duration', execution_time, tags={
            'mapping_type': mapping_type
        })

        self.metrics_collector.histogram('data_mapping_size', data_size, tags={
            'mapping_type': mapping_type
        })

    def trace_data_transformation(self,
                                execution_id: str,
                                source_data: Dict[str, Any],
                                target_data: Dict[str, Any],
                                mapping: DataMapping):
        """è®°å½•æ•°æ®è½¬æ¢è½¨è¿¹"""

        trace_record = {
            'execution_id': execution_id,
            'timestamp': datetime.now().isoformat(),
            'mapping_type': mapping.type,
            'source_data_hash': hash(str(source_data)),
            'target_data_hash': hash(str(target_data)),
            'source_fields': list(source_data.keys()) if isinstance(source_data, dict) else [],
            'target_fields': list(target_data.keys()) if isinstance(target_data, dict) else [],
            'mapping_description': mapping.description or ''
        }

        self.trace_logger.log_trace('data_transformation', trace_record)
```

### è°ƒè¯•å·¥å…·

```python
class DataMappingDebugger:
    """æ•°æ®æ˜ å°„è°ƒè¯•å·¥å…·"""

    def debug_mapping(self,
                     source_data: Dict[str, Any],
                     mapping: DataMapping,
                     context: ExecutionContext) -> Dict[str, Any]:
        """è°ƒè¯•æ•°æ®æ˜ å°„è¿‡ç¨‹"""

        debug_info = {
            'input': {
                'source_data': source_data,
                'mapping_config': mapping.__dict__,
                'context': context.to_dict()
            },
            'steps': [],
            'output': None,
            'errors': []
        }

        try:
            processor = DataMappingProcessor()

            # é€æ­¥æ‰§è¡Œå¹¶è®°å½•è¿‡ç¨‹
            if mapping.type == MappingType.FIELD_MAPPING:
                debug_info['steps'] = self._debug_field_mapping(
                    source_data, mapping, context
                )

            # æ‰§è¡Œå®Œæ•´è½¬æ¢
            result = processor.transform_data(source_data, mapping, context)
            debug_info['output'] = result

        except Exception as e:
            debug_info['errors'].append(str(e))
            debug_info['output'] = {}

        return debug_info

    def _debug_field_mapping(self, source_data, mapping, context):
        """è°ƒè¯•å­—æ®µæ˜ å°„è¿‡ç¨‹"""
        steps = []

        for field_mapping in mapping.field_mappings:
            step = {
                'source_field': field_mapping.source_field,
                'target_field': field_mapping.target_field,
                'required': field_mapping.required
            }

            try:
                # æå–æºå­—æ®µå€¼
                source_value = self._extract_field_for_debug(
                    source_data, field_mapping.source_field
                )
                step['source_value'] = source_value
                step['source_found'] = source_value is not None

                # åº”ç”¨è½¬æ¢
                if field_mapping.transform:
                    transformed_value = self._apply_transform_for_debug(
                        source_value, field_mapping.transform, context
                    )
                    step['transformed_value'] = transformed_value
                    step['transform_applied'] = True
                else:
                    step['transformed_value'] = source_value
                    step['transform_applied'] = False

                step['success'] = True

            except Exception as e:
                step['error'] = str(e)
                step['success'] = False

            steps.append(step)

        return steps
```

## ğŸ“‹ æœ€ä½³å®è·µ

### 1. æ˜ å°„è®¾è®¡åŸåˆ™

- **æ˜ç¡®æ€§**: æ¯ä¸ªæ˜ å°„éƒ½åº”è¯¥æœ‰æ¸…æ™°çš„æè¿°
- **å¹‚ç­‰æ€§**: ç›¸åŒè¾“å…¥åº”è¯¥äº§ç”Ÿç›¸åŒè¾“å‡º
- **é”™è¯¯å¤„ç†**: ä¼˜é›…å¤„ç†ç¼ºå¤±å­—æ®µå’Œç±»å‹é”™è¯¯
- **æ€§èƒ½è€ƒè™‘**: é¿å…å¤æ‚çš„åµŒå¥—è½¬æ¢

### 2. å­—æ®µè·¯å¾„è§„èŒƒ

```javascript
// æ¨èçš„å­—æ®µè·¯å¾„æ ¼å¼
"simple_field"                    // ç®€å•å­—æ®µ
"nested.field.path"              // åµŒå¥—å­—æ®µ
"array_field[0]"                 // æ•°ç»„å…ƒç´ 
"array_field[*]"                 // æ‰€æœ‰æ•°ç»„å…ƒç´ 
"$.complex.jsonpath.expression"   // å¤æ‚JSONPathè¡¨è¾¾å¼
```

### 3. è½¬æ¢è„šæœ¬è§„èŒƒ

```javascript
// è½¬æ¢è„šæœ¬æ¨¡æ¿
function transform(input, context) {
    // 1. è¾“å…¥éªŒè¯
    if (!input || typeof input !== 'object') {
        throw new Error('Invalid input data');
    }

    // 2. æ•°æ®å¤„ç†é€»è¾‘
    const processedData = processInput(input);

    // 3. æ„å»ºè¾“å‡º
    const output = {
        // ç»“æ„åŒ–è¾“å‡º
    };

    // 4. è¾“å‡ºéªŒè¯
    validateOutput(output);

    return output;
}

// è¾…åŠ©å‡½æ•°
function processInput(input) { /* ... */ }
function validateOutput(output) { /* ... */ }
```

### 4. é”™è¯¯å¤„ç†ç­–ç•¥

```python
# é”™è¯¯å¤„ç†æœ€ä½³å®è·µ
class DataMappingError(Exception):
    """æ•°æ®æ˜ å°„å¼‚å¸¸åŸºç±»"""
    pass

class FieldExtractionError(DataMappingError):
    """å­—æ®µæå–å¼‚å¸¸"""
    pass

class TransformationError(DataMappingError):
    """æ•°æ®è½¬æ¢å¼‚å¸¸"""
    pass

class ValidationError(DataMappingError):
    """æ•°æ®éªŒè¯å¼‚å¸¸"""
    pass

# åœ¨æ˜ å°„å¤„ç†å™¨ä¸­ä½¿ç”¨
try:
    result = self.transform_data(source_data, mapping, context)
except FieldExtractionError as e:
    # å­—æ®µæå–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼æˆ–è·³è¿‡
    self.logger.warning(f"Field extraction failed: {e}")
    result = self._apply_default_mapping(source_data)
except TransformationError as e:
    # è½¬æ¢å¤±è´¥ï¼Œå›é€€åˆ°ç›´æ¥æ˜ å°„
    self.logger.error(f"Transformation failed: {e}")
    result = source_data
except ValidationError as e:
    # éªŒè¯å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸åœæ­¢å·¥ä½œæµ
    self.logger.error(f"Output validation failed: {e}")
    raise
```

## ğŸ¯ æ€»ç»“

æ•°æ®æ˜ å°„ç³»ç»Ÿçš„å¼•å…¥å°†å½»åº•è§£å†³å·¥ä½œæµä¸­**æ•°æ®è½¬æ¢**çš„é—®é¢˜ï¼š

### âœ… **è§£å†³çš„æ ¸å¿ƒé—®é¢˜**
1. **æ•°æ®æ ¼å¼ä¸åŒ¹é…**: è‡ªåŠ¨è½¬æ¢ä¸åŒèŠ‚ç‚¹é—´çš„æ•°æ®æ ¼å¼
2. **å­—æ®µæ˜ å°„å¤æ‚**: æ”¯æŒçµæ´»çš„å­—æ®µè·¯å¾„å’Œè½¬æ¢è§„åˆ™
3. **ç¼ºä¹ç±»å‹å®‰å…¨**: åŸºäºèŠ‚ç‚¹è§„èŒƒè¿›è¡Œæ•°æ®éªŒè¯
4. **è°ƒè¯•å›°éš¾**: å®Œæ•´çš„æ•°æ®æµè½¬æ—¥å¿—å’Œè°ƒè¯•å·¥å…·

### ğŸš€ **å¸¦æ¥çš„ä»·å€¼**
1. **å¼€å‘æ•ˆç‡**: å¯è§†åŒ–çš„æ•°æ®æ˜ å°„é…ç½®
2. **ç³»ç»Ÿå¯é æ€§**: å¼ºç±»å‹éªŒè¯å’Œé”™è¯¯å¤„ç†
3. **è¿ç»´å‹å¥½**: è¯¦ç»†çš„ç›‘æ§å’Œè°ƒè¯•ä¿¡æ¯
4. **ä¸šåŠ¡çµæ´»æ€§**: æ”¯æŒå¤æ‚çš„ä¸šåŠ¡æ•°æ®è½¬æ¢é€»è¾‘

### ğŸ“ˆ **æœªæ¥æ‰©å±•**
1. **AIè¾…åŠ©æ˜ å°„**: åŸºäºæ•°æ®æ ·æœ¬è‡ªåŠ¨ç”Ÿæˆæ˜ å°„è§„åˆ™
2. **å¯è§†åŒ–ç¼–è¾‘å™¨**: æ‹–æ‹½å¼çš„æ•°æ®æ˜ å°„é…ç½®ç•Œé¢
3. **æ€§èƒ½ä¼˜åŒ–**: å¹¶è¡Œå¤„ç†å’Œæ™ºèƒ½ç¼“å­˜
4. **æ›´å¤šè½¬æ¢ç±»å‹**: æ”¯æŒæ›´å¤šçš„æ•°æ®è½¬æ¢æ¨¡å¼

è¿™ä¸ªæ•°æ®æ˜ å°„ç³»ç»Ÿå°†æˆä¸ºå·¥ä½œæµå¼•æ“çš„æ ¸å¿ƒèƒ½åŠ›ä¹‹ä¸€ï¼Œä¸ºæ„å»ºå¤æ‚çš„ä¸šåŠ¡è‡ªåŠ¨åŒ–æµç¨‹æä¾›å¼ºæœ‰åŠ›çš„åŸºç¡€æ”¯æ’‘ã€‚

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0
**åˆ›å»ºæ—¶é—´**: 2025-01-28
**ä½œè€…**: Claude Code
**çŠ¶æ€**: è®¾è®¡é˜¶æ®µ
**ä¸‹æ¬¡å®¡æŸ¥**: 2025-02-04
