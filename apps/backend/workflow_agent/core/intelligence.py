import sys
from pathlib import Path

# Add the backend path to sys.path to import shared modules
backend_path = Path(__file__).resolve().parent.parent.parent
if str(backend_path) not in sys.path:
    sys.path.insert(0, str(backend_path))

"""
Intelligence engines for Workflow Agent
Based on the MVP plan and architecture design
"""

import json
from typing import Any, Dict, List, Optional

import structlog
from langchain.prompts import PromptTemplate
from langchain_anthropic import ChatAnthropic
from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI

from core.config import settings
from core.mvp_models import (
    CapabilityAnalysis,
    Constraint,
    GapSeverity,
    Solution,
    SolutionReliability,
    SolutionType,
)
from core.prompt_engine import get_prompt_engine
from core.vector_store import get_node_knowledge_rag

logger = structlog.get_logger()


class LLMCapabilityScanner:
    def __init__(self, llm, rag_system):
        self.llm = llm
        self.rag = rag_system
        self.capability_library = {}  # Legacy support
        self.prompt_engine = get_prompt_engine()

        # LLM Prompts
        # Templates will be loaded on-demand via prompt_engine

    async def _call_llm(self, prompt: str, **kwargs) -> Dict[str, Any]:
        """Helper method to call LLM with error handling"""
        try:
            response = await self.llm.ainvoke([HumanMessage(content=prompt)])

            # Try to parse JSON response
            try:
                return json.loads(response.content)
            except json.JSONDecodeError:
                # If not JSON, return as text
                return {"response": response.content}

        except Exception as e:
            logger.error(f"LLM call failed: {e}")
            return {"error": str(e)}

    async def _extract_required_capabilities(self, requirements: Dict[str, Any]) -> List[str]:
        """Extract required capabilities using LLM"""
        context = {
            "complexity_preference": requirements.get("estimated_complexity", "medium"),
            "business_context": requirements.get("primary_goal", ""),
            "performance_requirements": requirements.get("performance_requirements", {}),
        }

        prompt_str = await self.prompt_engine.render_prompt(
            "capability_extraction",
            requirements=json.dumps(requirements, indent=2),
            context=json.dumps(context, indent=2),
        )
        result = await self._call_llm(prompt_str)

        return result.get("capabilities", [])

    async def _analyze_capability_gaps(
        self,
        required_capabilities: List[str],
        available_capabilities: List[str],
        rag_insights: Dict[str, Any],
    ) -> Dict[str, Any]:
        """Analyze capability gaps using LLM"""
        prompt_str = await self.prompt_engine.render_prompt(
            "gap_analysis",
            required_capabilities=json.dumps(required_capabilities),
            available_capabilities=json.dumps(available_capabilities),
            rag_insights=json.dumps(rag_insights, indent=2),
        )
        result = await self._call_llm(prompt_str)

        return result

    async def _generate_solutions(self, gap: str, requirements: Dict[str, Any]) -> List[Dict]:
        """Generate solutions for capability gap using LLM"""
        context = {
            "business_context": requirements.get("primary_goal", ""),
            "constraints": requirements.get("constraints", {}),
            "preferences": requirements.get("preferences", {}),
        }

        prompt_str = await self.prompt_engine.render_prompt(
            "solution_generation",
            gap=gap,
            requirements=json.dumps(requirements, indent=2),
            context=json.dumps(context, indent=2),
        )
        result = await self._call_llm(prompt_str)

        return result if isinstance(result, list) else []

    async def perform_capability_scan(self, requirements: Dict[str, Any]) -> CapabilityAnalysis:
        """
        Dynamic capability scanning using LLM and RAG
        """
        logger.info("Starting LLM-enhanced capability scan")

        # Step 1: Extract required capabilities using LLM
        required_capabilities = await self._extract_required_capabilities(requirements)

        # Step 2: Get RAG recommendations
        rag_recommendations = await self.rag.get_capability_recommendations(
            required_capabilities,
            context={
                "complexity_preference": requirements.get("estimated_complexity", "medium"),
                "business_context": requirements.get("primary_goal", ""),
                "performance_requirements": requirements.get("performance_requirements", {}),
            },
        )

        # Step 3: Combine static and RAG capabilities
        static_capabilities = list(self.capability_library.get("capability_matrix", {}).keys())
        rag_capabilities = []

        for cap_matches in rag_recommendations.get("capability_matches", {}).values():
            for match in cap_matches:
                if hasattr(match, "node_type") and match.node_type not in rag_capabilities:
                    rag_capabilities.append(match.node_type)

        available_capabilities = list(set(static_capabilities + rag_capabilities))

        # Step 4: Analyze gaps using LLM
        gap_analysis = await self._analyze_capability_gaps(
            required_capabilities, available_capabilities, rag_recommendations
        )

        capability_gaps = gap_analysis.get("gaps", [])

        # Safe handling of gap severity with fallback
        severity_data = gap_analysis.get("severity", {})
        gap_severity = {}
        for gap, severity in severity_data.items() if isinstance(severity_data, dict) else []:
            try:
                gap_severity[gap] = (
                    GapSeverity(severity) if isinstance(severity, str) else GapSeverity.MEDIUM
                )
            except (ValueError, TypeError):
                gap_severity[gap] = GapSeverity.MEDIUM  # Default fallback

        # Step 5: Generate solutions for each gap using LLM
        potential_solutions = {}
        for gap in capability_gaps:
            solutions_data = await self._generate_solutions(gap, requirements)
            solutions = []
            for sol_data in solutions_data[:5]:  # Top 5 solutions
                try:
                    if isinstance(sol_data, dict):
                        solution = Solution(
                            type=SolutionType(sol_data.get("type", "api_integration")),
                            complexity=sol_data.get("complexity", 5),
                            setup_time=sol_data.get("setup_time", "unknown"),
                            requires_user_action=sol_data.get("requires_user_action", "none"),
                            reliability=SolutionReliability(sol_data.get("reliability", "medium")),
                            description=sol_data.get("description", ""),
                        )
                        solutions.append(solution)
                except (ValueError, TypeError) as e:
                    logger.warning(f"Failed to create Solution from {sol_data}: {e}")
                    continue
            potential_solutions[gap] = solutions

        # Step 6: Calculate complexity scores (LLM + legacy fallback)
        complexity_scores = {}
        for cap in required_capabilities:
            if cap in self.capability_library.get("capability_matrix", {}):
                complexity_scores[cap] = self.capability_library["capability_matrix"][cap][
                    "complexity_score"
                ]
            else:
                # Use LLM to estimate complexity
                prompt_str = await self.prompt_engine.render_prompt(
                    "capability_complexity_estimation",
                    capability=cap,
                    context=json.dumps(requirements, indent=2),
                )
                complexity_result = await self._call_llm(prompt_str)
                try:
                    complexity_scores[cap] = float(complexity_result.get("response", "8"))
                except (ValueError, TypeError):
                    complexity_scores[cap] = 8.0  # Default

        # Create capability analysis result
        try:
            capability_analysis = CapabilityAnalysis(
                required_capabilities=required_capabilities,
                available_capabilities=available_capabilities,
                capability_gaps=capability_gaps,
                gap_severity=gap_severity,
                potential_solutions=potential_solutions,
                complexity_scores=complexity_scores,
            )
        except Exception as e:
            logger.error("Failed to create CapabilityAnalysis", error=str(e))
            # Create a dict with the same structure as fallback
            capability_analysis = {
                "required_capabilities": required_capabilities,
                "available_capabilities": available_capabilities,
                "capability_gaps": capability_gaps,
                "gap_severity": gap_severity,
                "potential_solutions": potential_solutions,
                "complexity_scores": complexity_scores,
                "rag_insights": {},
            }
            return capability_analysis

        # Add RAG insights
        capability_analysis.rag_insights = {
            "coverage_score": rag_recommendations.get("coverage_score", 0),
            "total_rag_matches": rag_recommendations.get("total_matches", 0),
            "missing_capabilities": rag_recommendations.get("missing_capabilities", []),
            "recommended_alternatives": rag_recommendations.get("alternatives", [])[:3],
            "confidence": "high"
            if rag_recommendations.get("coverage_score", 0) > 0.8
            else "medium",
        }

        logger.info(
            "LLM-enhanced capability scan completed",
            required=len(required_capabilities),
            gaps=len(capability_gaps),
            rag_coverage=rag_recommendations.get("coverage_score", 0),
        )

        return capability_analysis

    async def assess_complexity(self, capabilities: CapabilityAnalysis) -> Dict[str, Any]:
        """
        Comprehensive complexity assessment using LLM
        """
        logger.info("Assessing complexity with LLM")

        # Prepare data for LLM
        capabilities_data = {
            "required_capabilities": capabilities.required_capabilities,
            "capability_gaps": capabilities.capability_gaps,
            "complexity_scores": capabilities.complexity_scores,
            "gap_severity": {
                gap: severity.value for gap, severity in capabilities.gap_severity.items()
            },
        }

        # Use LLM for complexity assessment
        prompt_str = await self.prompt_engine.render_prompt(
            "complexity_assessment",
            capabilities=json.dumps(capabilities_data, indent=2),
            requirements="",  # You might want to pass original requirements here
            context="",
        )
        complexity_result = await self._call_llm(prompt_str)

        logger.info(
            "LLM complexity assessment completed",
            overall_score=complexity_result.get("overall_score", 0),
        )

        return complexity_result

    async def identify_constraints(self, analysis: Dict[str, Any]) -> List[Constraint]:
        """
        Identify constraints using LLM
        """
        logger.info("Identifying constraints with LLM")

        # Use LLM for constraint identification
        prompt_str = await self.prompt_engine.render_prompt(
            "constraint_identification",
            analysis=json.dumps(analysis, indent=2),
            requirements=json.dumps(analysis.get("requirements", {}), indent=2),
        )
        constraint_result = await self._call_llm(prompt_str)

        # Convert to Constraint objects
        constraints = []
        for constraint_data in constraint_result if isinstance(constraint_result, list) else []:
            try:
                constraints.append(
                    Constraint(
                        type=constraint_data.get("type", "unknown"),
                        description=constraint_data.get("description", ""),
                        severity=GapSeverity(constraint_data.get("severity", "medium")),
                        impact=constraint_data.get("impact", ""),
                    )
                )
            except (KeyError, ValueError) as e:
                logger.warning(f"Failed to parse constraint: {e}")
                continue

        logger.info("LLM constraint identification completed", count=len(constraints))
        return constraints


class IntelligentAnalyzer:
    """
    Complete intelligent analysis engine as per MVP plan
    Handles requirement parsing and capability scanning
    """

    def __init__(self):
        self.llm = self._setup_llm()
        self.capability_library = self._load_capability_library()
        self.historical_cases = self._load_historical_cases()
        self.rag = get_node_knowledge_rag()
        self.prompt_engine = get_prompt_engine()
        self.scanner = LLMCapabilityScanner(self.llm, self.rag)

    def _setup_llm(self):
        """Setup the language model based on configuration"""
        if settings.DEFAULT_MODEL_PROVIDER == "openai":
            return ChatOpenAI(
                model_name=settings.DEFAULT_MODEL_NAME,
                openai_api_key=settings.OPENAI_API_KEY,
                temperature=0.1,
            )
        elif settings.DEFAULT_MODEL_PROVIDER == "anthropic":
            return ChatAnthropic(
                model_name=settings.DEFAULT_MODEL_NAME,
                anthropic_api_key=settings.ANTHROPIC_API_KEY,
                temperature=0.1,
            )
        else:
            raise ValueError(f"Unsupported model provider: {settings.DEFAULT_MODEL_PROVIDER}")

    def _load_capability_library(self) -> Dict[str, Any]:
        """Load the complete capability library"""
        return {
            "native_nodes": {
                "triggers": ["email", "webhook", "cron", "manual", "slack", "file_watcher"],
                "ai_agents": [
                    "task_analyzer",
                    "data_integrator",
                    "report_generator",
                    "router_agent",
                    "content_analyzer",
                    "decision_maker",
                ],
                "external_integrations": [
                    "slack",
                    "notion",
                    "gmail",
                    "github",
                    "google_calendar",
                    "trello",
                    "asana",
                    "jira",
                    "dropbox",
                    "google_drive",
                ],
                "flow_controls": [
                    "if_else",
                    "loop",
                    "parallel",
                    "error_handling",
                    "retry",
                    "timeout",
                    "rate_limit",
                ],
                "memory_systems": [
                    "vector_store",
                    "knowledge_base",
                    "session_memory",
                    "cache",
                    "database",
                ],
                "transformations": [
                    "data_mapper",
                    "format_converter",
                    "aggregator",
                    "filter",
                    "sorter",
                    "validator",
                ],
            },
            "capability_matrix": {
                "email_monitoring": {
                    "complexity_score": 3,
                    "setup_time": "15åˆ†é’Ÿ",
                    "reliability": "high",
                    "alternatives": ["webhook", "manual_check"],
                    "dependencies": ["email_credentials"],
                },
                "ai_analysis": {
                    "complexity_score": 6,
                    "setup_time": "30-60åˆ†é’Ÿ",
                    "reliability": "medium",
                    "dependencies": ["openai_api", "prompt_templates"],
                    "alternatives": ["rule_based", "keyword_matching"],
                },
                "notion_integration": {
                    "complexity_score": 4,
                    "setup_time": "20åˆ†é’Ÿ",
                    "reliability": "high",
                    "dependencies": ["notion_api_key"],
                    "alternatives": ["airtable", "google_sheets"],
                },
                "slack_integration": {
                    "complexity_score": 3,
                    "setup_time": "15åˆ†é’Ÿ",
                    "reliability": "high",
                    "dependencies": ["slack_bot_token"],
                    "alternatives": ["discord", "teams"],
                },
                "github_integration": {
                    "complexity_score": 4,
                    "setup_time": "25åˆ†é’Ÿ",
                    "reliability": "high",
                    "dependencies": ["github_token"],
                    "alternatives": ["gitlab", "bitbucket"],
                },
                "customer_detection": {
                    "complexity_score": 7,
                    "setup_time": "1-2å°æ—¶",
                    "reliability": "medium",
                    "dependencies": ["training_data", "ai_model"],
                    "alternatives": ["keyword_filter", "regex_matching"],
                },
                "data_transformation": {
                    "complexity_score": 5,
                    "setup_time": "45åˆ†é’Ÿ",
                    "reliability": "high",
                    "dependencies": ["mapping_rules"],
                    "alternatives": ["manual_processing", "simple_copy"],
                },
            },
        }

    def _load_historical_cases(self) -> List[Dict[str, Any]]:
        """Load historical successful cases for pattern matching"""
        return [
            {
                "pattern": "customer_service_automation",
                "description": "é‚®ä»¶ç›‘æ§ + AIåˆ†æ + è‡ªåŠ¨å›å¤/äººå·¥è½¬æ¥",
                "success_rate": 0.85,
                "avg_complexity": 6,
                "common_issues": ["AIä¿¡å¿ƒåº¦è°ƒä¼˜", "é‚®ä»¶åˆ†ç±»å‡†ç¡®æ€§"],
                "best_practices": ["è®¾ç½®ä¿¡å¿ƒåº¦é˜ˆå€¼", "æ·»åŠ äººå·¥å®¡æ ¸æœºåˆ¶"],
            },
            {
                "pattern": "data_integration_pipeline",
                "description": "å®šæ—¶ä»»åŠ¡ + æ•°æ®æŠ“å– + è½¬æ¢ + å¤šç›®æ ‡è¾“å‡º",
                "success_rate": 0.92,
                "avg_complexity": 5,
                "common_issues": ["æ•°æ®æ ¼å¼ä¸ä¸€è‡´", "APIé™åˆ¶"],
                "best_practices": ["å¢é‡æ›´æ–°", "é”™è¯¯é‡è¯•æœºåˆ¶"],
            },
            {
                "pattern": "notification_system",
                "description": "äº‹ä»¶è§¦å‘ + æ¡ä»¶åˆ¤æ–­ + å¤šæ¸ é“é€šçŸ¥",
                "success_rate": 0.95,
                "avg_complexity": 4,
                "common_issues": ["é€šçŸ¥å»é‡", "æ—¶åŒºå¤„ç†"],
                "best_practices": ["é€šçŸ¥æ¨¡æ¿åŒ–", "ç”¨æˆ·åå¥½è®¾ç½®"],
            },
        ]

    async def parse_requirements(self, user_input: str) -> Dict[str, Any]:
        """
        Deep requirement parsing with intent understanding
        å®Œæ•´å®ç° - ä¸é•¿æœŸæ„¿æ™¯ä¸€è‡´
        """
        logger.info("Starting deep requirement parsing", input=user_input)

        # Use centralized prompt templates
        prompt_str = await self.prompt_engine.render_prompt(
            "analyze_requirement_user", description=user_input
        )

        messages = [HumanMessage(content=prompt_str)]

        try:
            response = await self.llm.ainvoke(messages)
            content = response.content if hasattr(response, "content") else str(response)
            analysis = json.loads(content)

            # å¢å¼ºåˆ†æç»“æœ
            analysis["confidence"] = self._calculate_confidence(analysis)
            analysis["category"] = self._categorize_requirement(analysis)
            analysis["estimated_complexity"] = self._estimate_complexity(analysis)

            logger.info("Requirement parsing completed", analysis=analysis)
            return analysis

        except Exception as e:
            logger.error("Failed to parse requirements", error=str(e))
            # Return fallback analysis matching new template structure
            return {
                "requirement_analysis": {
                    "primary_goal": "åŸºæœ¬å·¥ä½œæµè‡ªåŠ¨åŒ–",
                    "secondary_goals": ["æé«˜æ•ˆç‡"],
                    "success_criteria": ["ç³»ç»Ÿæ­£å¸¸è¿è¡Œ"],
                    "business_value": "è‡ªåŠ¨åŒ–å¤„ç†ç”¨æˆ·éœ€æ±‚",
                    "confidence_level": 0.3,
                },
                "technical_requirements": {
                    "triggers": [
                        {
                            "type": "manual",
                            "description": "æ‰‹åŠ¨è§¦å‘",
                            "frequency": "æŒ‰éœ€",
                            "conditions": "ç”¨æˆ·å¯åŠ¨",
                        }
                    ],
                    "main_operations": [
                        {
                            "operation": "æ•°æ®å¤„ç†",
                            "description": "åŸºæœ¬æ•°æ®å¤„ç†",
                            "complexity": "medium",
                            "ai_required": False,
                        }
                    ],
                    "data_flow": {
                        "input_sources": ["ç”¨æˆ·è¾“å…¥"],
                        "processing_steps": ["åŸºæœ¬å¤„ç†"],
                        "output_destinations": ["ç³»ç»Ÿè¾“å‡º"],
                        "data_transformations": ["åŸºæœ¬è½¬æ¢"],
                    },
                    "integrations": [],
                    "performance_requirements": {
                        "volume": "ä½é‡",
                        "latency": "æ­£å¸¸",
                        "availability": "æ ‡å‡†",
                    },
                },
                "constraints": {
                    "technical_constraints": [],
                    "business_constraints": [],
                    "resource_constraints": ["éœ€è¦æ›´å¤šä¿¡æ¯"],
                    "compliance_requirements": [],
                },
                "complexity_assessment": {
                    "overall_complexity": "medium",
                    "technical_complexity": 5,
                    "business_complexity": 3,
                    "integration_complexity": 2,
                    "complexity_drivers": ["ä¿¡æ¯ä¸è¶³"],
                },
                "risk_analysis": {
                    "implementation_risks": [],
                    "operational_risks": [],
                    "ambiguities": [{"area": "éœ€æ±‚ç†è§£", "question": "éœ€è¦æ›´è¯¦ç»†çš„éœ€æ±‚æè¿°", "impact": "å½±å“å‡†ç¡®åˆ†æ"}],
                },
                "recommendations": {
                    "immediate_clarifications": ["è¯·æä¾›æ›´è¯¦ç»†çš„éœ€æ±‚æè¿°"],
                    "alternative_approaches": ["ä»ç®€å•åŠŸèƒ½å¼€å§‹"],
                    "success_factors": ["æ˜ç¡®éœ€æ±‚", "é€æ­¥å®ç°"],
                },
                "metadata": {
                    "category": "automation",
                    "estimated_timeline": "1-2å¤©",
                    "skill_requirements": ["åŸºæœ¬é…ç½®"],
                    "similar_patterns": ["åŸºæœ¬è‡ªåŠ¨åŒ–"],
                },
            }

    def match_historical_cases(self, requirements: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        Match against historical successful cases
        å®Œæ•´å®ç° - ä¸é•¿æœŸæ„¿æ™¯ä¸€è‡´
        """
        logger.info("Matching historical cases")

        matches = []

        for case in self.historical_cases:
            similarity_score = self._calculate_similarity(requirements, case)
            if similarity_score > 0.3:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                matches.append(
                    {
                        "case": case,
                        "similarity_score": similarity_score,
                        "applicable_practices": case["best_practices"],
                        "potential_issues": case["common_issues"],
                    }
                )

        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        matches.sort(key=lambda x: x["similarity_score"], reverse=True)

        logger.info("Historical case matching completed", matches_count=len(matches))
        return matches

    async def perform_capability_scan(self, requirements: Dict[str, Any]) -> CapabilityAnalysis:
        """
        Dynamic capability scanning with real-time assessment and RAG enhancement
        """
        return await self.scanner.perform_capability_scan(requirements)

    async def assess_complexity(self, capabilities: CapabilityAnalysis) -> Dict[str, Any]:
        """
        Comprehensive complexity assessment
        """
        return await self.scanner.assess_complexity(capabilities)

    async def identify_constraints(self, analysis: Dict[str, Any]) -> List[Constraint]:
        """
        Identify technical and business constraints
        """
        return await self.scanner.identify_constraints(analysis)

    # Helper methods
    def _calculate_confidence(self, analysis: Dict[str, Any]) -> float:
        """Calculate confidence score for requirement analysis"""
        confidence = 0.5  # Base confidence

        # å¢åŠ ä¿¡å¿ƒåº¦çš„å› ç´ 
        if analysis.get("primary_goal"):
            confidence += 0.2
        if analysis.get("triggers"):
            confidence += 0.1
        if analysis.get("integrations"):
            confidence += 0.1
        if len(analysis.get("ambiguities", [])) == 0:
            confidence += 0.1

        return min(confidence, 1.0)

    def _categorize_requirement(self, analysis: Dict[str, Any]) -> str:
        """Categorize the requirement type"""
        if "notification" in analysis.get("primary_goal", "").lower():
            return "notification"
        elif "data" in analysis.get("primary_goal", "").lower():
            return "data_processing"
        elif "customer" in analysis.get("primary_goal", "").lower():
            return "customer_service"
        else:
            return "automation"

    def _estimate_complexity(self, analysis: Dict[str, Any]) -> int:
        """Estimate overall complexity score"""
        complexity = 3  # Base complexity

        complexity += len(analysis.get("integrations", [])) * 2
        complexity += len(analysis.get("human_intervention", [])) * 1
        complexity += len(analysis.get("complexity_indicators", [])) * 1

        return min(complexity, 10)

    def _calculate_similarity(self, requirements: Dict[str, Any], case: Dict[str, Any]) -> float:
        """Calculate similarity between requirements and historical case"""
        # ç®€åŒ–çš„ç›¸ä¼¼åº¦è®¡ç®—
        similarity = 0.0

        # æ¯”è¾ƒä¸»è¦ç›®æ ‡
        if requirements.get("category") == case.get("pattern"):
            similarity += 0.5

        # æ¯”è¾ƒå¤æ‚åº¦
        req_complexity = requirements.get("estimated_complexity", 5)
        case_complexity = case.get("avg_complexity", 5)
        complexity_diff = abs(req_complexity - case_complexity)
        if complexity_diff <= 2:
            similarity += 0.3

        # æ¯”è¾ƒé›†æˆæ•°é‡
        req_integrations = len(requirements.get("integrations", []))
        # ä¼°ç®—å†å²æ¡ˆä¾‹çš„é›†æˆæ•°é‡
        case_integrations = case.get("avg_complexity", 5) // 2
        if abs(req_integrations - case_integrations) <= 1:
            similarity += 0.2

        return similarity


class IntelligentNegotiator:
    """
    Intelligent negotiation engine for multi-round requirement refinement
    å®Œæ•´å®ç° - ä¸é•¿æœŸæ„¿æ™¯ä¸€è‡´
    """

    def __init__(self):
        self.llm_client = self._setup_llm()
        self.prompt_engine = get_prompt_engine()
        self.negotiation_patterns = self._load_negotiation_patterns()
        self.rag = get_node_knowledge_rag()

    def _setup_llm(self):
        """Setup the language model based on configuration"""
        if settings.DEFAULT_MODEL_PROVIDER == "openai":
            return ChatOpenAI(
                model_name=settings.DEFAULT_MODEL_NAME,
                openai_api_key=settings.OPENAI_API_KEY,
                temperature=0.1,
            )
        elif settings.DEFAULT_MODEL_PROVIDER == "anthropic":
            return ChatAnthropic(
                model_name=settings.DEFAULT_MODEL_NAME,
                anthropic_api_key=settings.ANTHROPIC_API_KEY,
                temperature=0.1,
            )
        else:
            raise ValueError(f"Unsupported model provider: {settings.DEFAULT_MODEL_PROVIDER}")

    def _load_negotiation_patterns(self) -> Dict[str, Any]:
        """Load negotiation patterns for different scenarios"""
        return {
            "capability_gap_negotiation": {
                "high_severity": {
                    "approach": "solution_comparison",
                    "tone": "consultative",
                    "focus": "tradeoffs_and_alternatives",
                },
                "medium_severity": {
                    "approach": "guided_selection",
                    "tone": "advisory",
                    "focus": "complexity_vs_benefit",
                },
                "low_severity": {
                    "approach": "simple_choice",
                    "tone": "informative",
                    "focus": "quick_decision",
                },
            },
            "complexity_negotiation": {
                "high_complexity": {
                    "approach": "phased_implementation",
                    "tone": "collaborative",
                    "focus": "risk_mitigation",
                },
                "medium_complexity": {
                    "approach": "optimization_suggestions",
                    "tone": "supportive",
                    "focus": "efficiency_gains",
                },
            },
        }

    async def generate_contextual_questions(
        self,
        gaps: List[str],
        capability_analysis: CapabilityAnalysis,
        history: List[Dict[str, Any]] = None,
    ) -> List[str]:
        """
        Generate context-aware negotiation questions
        å®Œæ•´å®ç° - ä¸é•¿æœŸæ„¿æ™¯ä¸€è‡´
        """
        logger.info("Generating contextual questions", gaps=gaps)

        if history is None:
            history = []

        questions = []

        for gap in gaps:
            severity = capability_analysis["gap_severity"].get(gap, GapSeverity.MEDIUM)
            solutions = capability_analysis["potential_solutions"].get(gap, [])

            if severity == GapSeverity.HIGH or severity == GapSeverity.CRITICAL:
                # é«˜ä¸¥é‡æ€§ç¼ºå£éœ€è¦è¯¦ç»†åå•†
                question = await self._generate_high_severity_question(gap, solutions, history)
                questions.append(question)
            elif severity == GapSeverity.MEDIUM:
                # ä¸­ç­‰ä¸¥é‡æ€§æä¾›é€‰æ‹©
                question = await self._generate_medium_severity_question(gap, solutions, history)
                questions.append(question)
            else:
                # ä½ä¸¥é‡æ€§ç®€å•è¯¢é—®
                question = await self._generate_low_severity_question(gap, solutions, history)
                questions.append(question)

        # å»é‡å’Œæ’åº
        questions = list(set(questions))
        questions.sort(key=lambda x: self._calculate_question_priority(x, capability_analysis))

        logger.info("Generated contextual questions", count=len(questions))
        return questions[:5]  # æœ€å¤šè¿”å›5ä¸ªé—®é¢˜

    async def present_tradeoff_analysis(
        self, solutions: List[Solution], context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Present comprehensive tradeoff analysis
        å®Œæ•´å®ç° - ä¸é•¿æœŸæ„¿æ™¯ä¸€è‡´
        """
        logger.info("Presenting tradeoff analysis", solutions_count=len(solutions))

        # åˆ›å»ºå¯¹æ¯”è¡¨
        comparison_table = self._create_comparison_table(solutions)

        # ç”Ÿæˆæ¨è
        recommendation = await self._generate_recommendation(solutions, context)

        # é£é™©åˆ†æ
        risk_analysis = self._analyze_risks(solutions)

        # æˆæœ¬æ•ˆç›Šåˆ†æ
        cost_benefit = self._calculate_cost_benefit(solutions)

        tradeoff_analysis = {
            "comparison_table": comparison_table,
            "recommendation": recommendation,
            "risk_analysis": risk_analysis,
            "cost_benefit": cost_benefit,
            "decision_framework": self._create_decision_framework(solutions),
        }

        logger.info("Tradeoff analysis completed")
        return tradeoff_analysis

    async def process_negotiation_round(
        self, user_input: str, context: Dict[str, Any], history: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Process a single negotiation round
        å®Œæ•´å®ç° - ä¸é•¿æœŸæ„¿æ™¯ä¸€è‡´
        """
        logger.info("Processing negotiation round", input=user_input)

        # åˆ†æç”¨æˆ·å›åº”
        response_analysis = await self._analyze_user_response(user_input, context)

        # æ›´æ–°ä¸Šä¸‹æ–‡
        updated_context = self._update_context(context, response_analysis)

        # ç¡®å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨
        next_action = self._determine_next_action(response_analysis, history)

        # ç”Ÿæˆå“åº”
        if next_action == "ask_clarification":
            response = await self._generate_clarification_question(
                response_analysis, updated_context
            )
        elif next_action == "present_alternatives":
            response = await self._present_alternatives(response_analysis, updated_context)
        elif next_action == "finalize_agreement":
            response = await self._finalize_agreement(response_analysis, updated_context)
        else:
            response = await self._generate_default_response(response_analysis, updated_context)

        negotiation_result = {
            "user_input": user_input,
            "response_analysis": response_analysis,
            "updated_context": updated_context,
            "next_action": next_action,
            "response": response,
            "negotiation_complete": next_action == "finalize_agreement",
        }

        # If negotiation is complete, generate final requirements
        if next_action == "finalize_agreement":
            # Extract original requirements from context or use user input
            original_requirements = updated_context.get("original_requirements", "")
            if not original_requirements:
                # Try to get from history
                if history:
                    original_requirements = history[0].get("user_response", "")
                if not original_requirements:
                    original_requirements = user_input

            # Generate final requirements
            final_requirements = self._generate_final_requirements(
                {"original_requirements": original_requirements}, []
            )

            negotiation_result["final_requirements"] = final_requirements
            negotiation_result["confidence_score"] = 0.8

        logger.info("Negotiation round processed", action=next_action)
        return negotiation_result

    def validate_agreements(self, decisions: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Validate user agreements for feasibility
        å®Œæ•´å®ç° - ä¸é•¿æœŸæ„¿æ™¯ä¸€è‡´
        """
        logger.info("Validating agreements", decisions_count=len(decisions))

        validation_results = {
            "feasible": True,
            "conflicts": [],
            "missing_decisions": [],
            "risk_factors": [],
            "recommendations": [],
        }

        # æ£€æŸ¥å†³ç­–ä¸€è‡´æ€§
        conflicts = self._check_decision_conflicts(decisions)
        validation_results["conflicts"] = conflicts

        # æ£€æŸ¥ç¼ºå¤±å†³ç­–
        missing = self._check_missing_decisions(decisions)
        validation_results["missing_decisions"] = missing

        # é£é™©è¯„ä¼°
        risks = self._assess_decision_risks(decisions)
        validation_results["risk_factors"] = risks

        # å¯è¡Œæ€§æ£€æŸ¥
        if conflicts or missing:
            validation_results["feasible"] = False

        # ç”Ÿæˆå»ºè®®
        recommendations = self._generate_validation_recommendations(validation_results)
        validation_results["recommendations"] = recommendations

        logger.info("Agreement validation completed", feasible=validation_results["feasible"])
        return validation_results

    def optimize_requirements(self, agreements: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Optimize requirements based on agreements
        å®Œæ•´å®ç° - ä¸é•¿æœŸæ„¿æ™¯ä¸€è‡´
        """
        logger.info("Optimizing requirements", agreements_count=len(agreements))

        # æ•´åˆå†³ç­–
        consolidated_decisions = self._consolidate_decisions(agreements)

        # ä¼˜åŒ–å»ºè®®
        optimizations = self._identify_optimizations(consolidated_decisions)

        # ç”Ÿæˆæœ€ç»ˆéœ€æ±‚
        final_requirements = self._generate_final_requirements(
            consolidated_decisions, optimizations
        )

        # ç½®ä¿¡åº¦è¯„ä¼°
        confidence_score = self._calculate_confidence_score(agreements)

        optimization_result = {
            "consolidated_decisions": consolidated_decisions,
            "optimizations": optimizations,
            "final_requirements": final_requirements,
            "confidence_score": confidence_score,
            "implementation_plan": self._create_implementation_plan(final_requirements),
        }

        logger.info("Requirements optimization completed", confidence=confidence_score)
        return optimization_result

    # Helper methods for negotiation
    async def _generate_high_severity_question(
        self, gap: str, solutions: List[Solution], history: List[Dict[str, Any]]
    ) -> str:
        """Generate question for high severity capability gap using templates"""

        # Use the negotiation engine prompt template
        system_prompt = await self.prompt_engine.render_prompt("negotiation_engine.j2")

        # Create context for the negotiation prompt
        context = {
            "consultation_type": "gap_resolution",
            "gap": gap,
            "solutions": [
                {
                    "option_id": f"A{i}",
                    "title": sol["description"],
                    "complexity": sol["complexity"],
                    "setup_time": sol["setup_time"],
                    "reliability": sol["reliability"],
                }
                for i, sol in enumerate(solutions[:3])
            ],
            "severity": "high",
            "history_length": len(history),
        }

        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(
                content=f"Generate consultation for high severity gap: {gap}. Solutions: {json.dumps(context['solutions'])}"
            ),
        ]

        try:
            response = await self.llm_client.ainvoke(messages)
            content = response.content if hasattr(response, "content") else str(response)

            # Try to parse as JSON for structured response
            try:
                result = json.loads(content.strip())
                if "guided_questions" in result and result["guided_questions"]:
                    return result["guided_questions"][0]["question"]
            except:
                pass

            return content.strip()

        except Exception as e:
            logger.warning(f"Failed to generate contextual question: {e}")
            # Fallback to simple question
            return f"å…³é”®èƒ½åŠ›ç¼ºå£ï¼š{gap}ã€‚æˆ‘ä»¬éœ€è¦æ‰¾åˆ°è§£å†³æ–¹æ¡ˆï¼Œæ‚¨æœ‰ä»€ä¹ˆå…·ä½“è¦æ±‚å—ï¼Ÿ"

    async def _generate_medium_severity_question(
        self, gap: str, solutions: List[Solution], history: List[Dict[str, Any]]
    ) -> str:
        """Generate question for medium severity capability gap using templates"""

        # Use the negotiation engine prompt template
        system_prompt = await self.prompt_engine.render_prompt("negotiation_engine")

        context = {
            "consultation_type": "requirement_clarification",
            "gap": gap,
            "solutions": [
                {
                    "option_id": f"B{i}",
                    "title": sol["description"],
                    "complexity": sol["complexity"],
                    "setup_time": sol["setup_time"],
                }
                for i, sol in enumerate(solutions[:2])
            ],
            "severity": "medium",
        }

        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(
                content=f"Generate consultation for medium severity gap: {gap}. Solutions: {json.dumps(context['solutions'])}"
            ),
        ]

        try:
            response = await self.llm_client.ainvoke(messages)
            content = response.content if hasattr(response, "content") else str(response)

            try:
                result = json.loads(content.strip())
                if "guided_questions" in result and result["guided_questions"]:
                    return result["guided_questions"][0]["question"]
            except:
                pass

            return content.strip()

        except Exception as e:
            logger.warning(f"Failed to generate medium severity question: {e}")
            return f"éœ€è¦å®ç°ï¼š{gap}ã€‚æ‚¨æœ‰åå¥½çš„å®ç°æ–¹å¼å—ï¼Ÿ"

    async def _generate_low_severity_question(
        self, gap: str, solutions: List[Solution], history: List[Dict[str, Any]]
    ) -> str:
        """Generate question for low severity capability gap using templates"""

        try:
            # Simple template-based generation for low severity
            if not solutions:
                return f"å¯é€‰åŠŸèƒ½ï¼š{gap}ã€‚æ˜¯å¦éœ€è¦åŒ…å«ï¼Ÿ"

            # Use simplest solution
            simple_solution = min(solutions, key=lambda x: x["complexity"])
            return f"å¯é€‰åŠŸèƒ½ï¼š{gap}ã€‚å»ºè®®ä½¿ç”¨{simple_solution['description']}ï¼Œæ‚¨åŒæ„å—ï¼Ÿ"

        except Exception as e:
            logger.warning(f"Failed to generate low severity question: {e}")
            return f"å¯é€‰åŠŸèƒ½ï¼š{gap}ã€‚ï¿½ï¿½å¦éœ€è¦åŒ…å«ï¼Ÿ"

    def _calculate_question_priority(
        self, question: str, capability_analysis: CapabilityAnalysis
    ) -> int:
        """Calculate priority for question ordering"""
        priority = 0

        # åŸºäºä¸¥é‡æ€§
        if "å…³é”®èƒ½åŠ›ç¼ºå£" in question:
            priority += 100
        elif "éœ€è¦å®ç°" in question:
            priority += 50
        elif "å¯é€‰åŠŸèƒ½" in question:
            priority += 10

        # åŸºäºå¤æ‚åº¦
        if "å¤æ‚åº¦" in question:
            # æå–å¤æ‚åº¦æ•°å­—
            import re

            complexity_match = re.search(r"å¤æ‚åº¦: (\d+)", question)
            if complexity_match:
                complexity = int(complexity_match.group(1))
                priority += complexity * 5

        return priority

    def _create_comparison_table(self, solutions: List[Solution]) -> Dict[str, Any]:
        """Create comparison table for solutions"""
        table = {"headers": ["è§£å†³æ–¹æ¡ˆ", "å¤æ‚åº¦", "è®¾ç½®æ—¶é—´", "å¯é æ€§", "ç”¨æˆ·æ“ä½œ"], "rows": []}

        for solution in solutions:
            row = [
                solution["description"],
                f"{solution['complexity']}/10",
                solution["setup_time"],
                solution["reliability"],
                solution["requires_user_action"],
            ]
            table["rows"].append(row)

        return table

    async def _generate_recommendation(
        self, solutions: List[Solution], context: Dict[str, Any]
    ) -> str:
        """Generate intelligent recommendation with RAG enhancement"""
        if not solutions:
            return "æš‚æ— å¯ç”¨è§£å†³æ–¹æ¡ˆ"

        # Get RAG insights for solution recommendation
        try:
            # Extract solution types for RAG search
            solution_types = [sol["type"] for sol in solutions]
            search_query = f"best practices for {', '.join(solution_types)} implementation"

            rag_insights = await self.rag.vector_store.similarity_search(search_query, k=3)

            # Combine traditional logic with RAG insights
            user_skill_level = context.get("user_skill_level", "medium")
            time_constraint = context.get("time_constraint", "medium")

            if user_skill_level == "low" or time_constraint == "high":
                # æ¨èç®€å•æ–¹æ¡ˆ
                simple_solution = min(solutions, key=lambda x: x["complexity"])
                recommendation = f"æ¨èï¼š{simple_solution['description']} - ç®€å•æ˜“ç”¨ï¼Œå¿«é€Ÿå®ç°"
            else:
                # æ¨èå¹³è¡¡æ–¹æ¡ˆ
                balanced_solution = min(
                    solutions,
                    key=lambda x: x["complexity"] + (10 - (8 if x["reliability"] == "high" else 5)),
                )
                recommendation = f"æ¨èï¼š{balanced_solution['description']} - å¹³è¡¡å¤æ‚åº¦å’Œå¯é æ€§"

            # Add RAG insights if available
            if rag_insights and rag_insights[0].metadata.get("score") > 0.6:
                best_practice = rag_insights[0].page_content[:100] + "..."
                recommendation += f"\n\nğŸ’¡ æœ€ä½³å®è·µå»ºè®®ï¼š{best_practice}"

            return recommendation

        except Exception as e:
            logger.warning("RAG recommendation enhancement failed", error=str(e))
            # Fallback to simple logic
            simple_solution = min(solutions, key=lambda x: x["complexity"])
            return f"æ¨èï¼š{simple_solution['description']} - ç®€å•æ˜“ç”¨ï¼Œå¿«é€Ÿå®ç°"

    def _analyze_risks(self, solutions: List[Solution]) -> List[str]:
        """Analyze risks in solutions"""
        risks = []

        for solution in solutions:
            if solution["reliability"] == SolutionReliability.LOW:
                risks.append(f"ä½å¯é æ€§é£é™©ï¼š{solution['description']}")
            if solution["complexity"] > 8:
                risks.append(f"é«˜å¤æ‚åº¦é£é™©ï¼š{solution['description']}")

        return risks

    def _calculate_cost_benefit(self, solutions: List[Solution]) -> Dict[str, Any]:
        """Calculate cost-benefit analysis"""
        if not solutions:
            return {"total_cost": 0, "expected_benefit": 0, "roi": 0}

        # ç®€åŒ–çš„æˆæœ¬æ•ˆç›Šè®¡ç®—
        avg_complexity = sum(s["complexity"] for s in solutions) / len(solutions)
        avg_reliability = sum(8 if s["reliability"] == "high" else 5 for s in solutions) / len(
            solutions
        )

        cost_score = avg_complexity * 10  # å‡è®¾å¤æ‚åº¦è½¬æ¢ä¸ºæˆæœ¬
        benefit_score = avg_reliability * 12  # å‡è®¾å¯é æ€§è½¬æ¢ä¸ºæ•ˆç›Š

        return {
            "cost_score": cost_score,
            "benefit_score": benefit_score,
            "roi": (benefit_score - cost_score) / cost_score if cost_score > 0 else 0,
        }

    def _create_decision_framework(self, solutions: List[Solution]) -> Dict[str, Any]:
        """Create decision framework for users"""
        return {
            "decision_criteria": [
                "å®ç°å¤æ‚åº¦",
                "è®¾ç½®æ—¶é—´",
                "é•¿æœŸç»´æŠ¤æˆæœ¬",
                "å¯é æ€§è¦æ±‚",
                "å›¢é˜ŸæŠ€èƒ½åŒ¹é…",
            ],
            "weight_suggestions": {
                "å¿«é€ŸåŸå‹": {"complexity": 0.5, "time": 0.3, "reliability": 0.2},
                "ç”Ÿäº§ç¯å¢ƒ": {"complexity": 0.2, "time": 0.2, "reliability": 0.6},
                "é•¿æœŸä½¿ç”¨": {"complexity": 0.3, "time": 0.1, "reliability": 0.6},
            },
        }

    # Additional helper methods would be implemented here
    async def _analyze_user_response(
        self, user_input: str, context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """Analyze user response to determine intent"""
        # Simplified analysis
        lower_input = user_input.lower()

        # Determine intent based on keywords
        if any(word in lower_input for word in ["é€‰æ‹©", "è¦", "ç”¨", "æˆ‘é€‰æ‹©", "æˆ‘è¦"]):
            intent = "selection"
        elif any(word in lower_input for word in ["å¥½", "æ˜¯", "å¯¹", "ç¡®è®¤", "åŒæ„"]):
            intent = "confirmation"
        elif any(word in lower_input for word in ["å®Œæˆ", "ç»“æŸ", "å¤Ÿäº†", "å¯ä»¥äº†"]):
            intent = "agreement"
        else:
            intent = "clarification"

        return {
            "intent": intent,
            "confidence": 0.8,
            "extracted_preferences": {},
            "sentiment": "positive",
            "user_input": user_input,  # Include the actual user input
        }

    def _update_context(self, context: Dict[str, Any], analysis: Dict[str, Any]) -> Dict[str, Any]:
        """Update negotiation context"""
        updated = context.copy()
        updated["last_response"] = analysis
        return updated

    def _determine_next_action(
        self, analysis: Dict[str, Any], history: List[Dict[str, Any]]
    ) -> str:
        """Determine next negotiation action"""
        # Check if we have enough rounds of negotiation
        if len(history) > 3:
            return "finalize_agreement"

        # Check if user provided substantial requirements
        user_input = analysis.get("user_input", "").strip()
        if len(user_input) > 50:  # Substantial input
            return "finalize_agreement"

        # Check intent
        intent = analysis.get("intent", "")
        if intent == "selection":
            return "present_alternatives"
        elif intent in ["confirmation", "agreement"]:
            return "finalize_agreement"
        else:
            return "ask_clarification"

    async def _generate_clarification_question(
        self, analysis: Dict[str, Any], context: Dict[str, Any]
    ) -> str:
        """Generate clarification question"""
        return "è¯·é—®æ‚¨éœ€è¦æˆ‘æ¾„æ¸…å“ªä¸ªæ–¹é¢çš„ä¿¡æ¯ï¼Ÿ"

    async def _present_alternatives(self, analysis: Dict[str, Any], context: Dict[str, Any]) -> str:
        """Present alternative solutions"""
        return "åŸºäºæ‚¨çš„é€‰æ‹©ï¼Œæˆ‘ä¸ºæ‚¨å‡†å¤‡äº†å‡ ä¸ªå¤‡é€‰æ–¹æ¡ˆ..."

    async def _finalize_agreement(self, analysis: Dict[str, Any], context: Dict[str, Any]) -> str:
        """Finalize negotiation agreement"""
        return "å¾ˆå¥½ï¼Œæˆ‘ä»¬è¾¾æˆäº†ä¸€è‡´ã€‚è®©æˆ‘æ€»ç»“ä¸€ä¸‹æˆ‘ä»¬çš„å†³å®š..."

    async def _generate_default_response(
        self, analysis: Dict[str, Any], context: Dict[str, Any]
    ) -> str:
        """Generate default response"""
        return "æˆ‘ç†è§£æ‚¨çš„è§‚ç‚¹ï¼Œè®©æˆ‘ä»¬ç»§ç»­è®¨è®º..."

    def _check_decision_conflicts(self, decisions: List[Dict[str, Any]]) -> List[str]:
        """Check for conflicts in decisions"""
        return []  # Simplified implementation

    def _check_missing_decisions(self, decisions: List[Dict[str, Any]]) -> List[str]:
        """Check for missing critical decisions"""
        return []  # Simplified implementation

    def _assess_decision_risks(self, decisions: List[Dict[str, Any]]) -> List[str]:
        """Assess risks in decisions"""
        return []  # Simplified implementation

    def _generate_validation_recommendations(self, validation: Dict[str, Any]) -> List[str]:
        """Generate validation recommendations"""
        return []  # Simplified implementation

    def _consolidate_decisions(self, agreements: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Consolidate all decisions"""
        # Extract the original requirements from the first agreement or negotiation context
        original_requirements = ""

        # Look for original requirements in agreements
        for agreement in agreements:
            if "original_requirements" in agreement:
                original_requirements = agreement["original_requirements"]
                break
            # Also check if there's a context with original requirements
            if "context" in agreement:
                context = agreement["context"]
                if isinstance(context, dict) and "original_requirements" in context:
                    original_requirements = context["original_requirements"]
                    break

        # If no original requirements found, try to extract from first agreement
        if not original_requirements and agreements:
            first_agreement = agreements[0]
            if "user_input" in first_agreement:
                original_requirements = first_agreement["user_input"]

        return {
            "original_requirements": original_requirements,
            "agreements": agreements,
            "decision_count": len(agreements),
        }

    def _identify_optimizations(self, decisions: Dict[str, Any]) -> List[str]:
        """Identify optimization opportunities"""
        return []  # Simplified implementation

    def _generate_final_requirements(
        self, decisions: Dict[str, Any], optimizations: List[str]
    ) -> str:
        """Generate final requirements"""
        # Extract the original requirements from decisions
        original_requirements = decisions.get("original_requirements", "")

        # If we have original requirements, use them
        if original_requirements and len(original_requirements.strip()) > 0:
            return original_requirements

        # Otherwise, generate a basic requirement
        return "åˆ›å»ºä¸€ä¸ªåŸºæœ¬çš„å·¥ä½œæµç¨‹æ¥å¤„ç†ç”¨æˆ·éœ€æ±‚"

    def _calculate_confidence_score(self, agreements: List[Dict[str, Any]]) -> float:
        """Calculate confidence score for agreements"""
        return 0.8  # Simplified implementation

    def _create_implementation_plan(self, requirements: str) -> Dict[str, Any]:
        """Create implementation plan"""
        return {"phases": [], "timeline": "", "resources": []}  # Simplified implementation
