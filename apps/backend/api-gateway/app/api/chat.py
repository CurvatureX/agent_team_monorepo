"""
æ–°çš„ Chat API - æ”¯æŒä¸‰ç§è¿”å›ç±»å‹ï¼šai message, workflow, error
åŸºäºæ–°çš„ workflow_agent.proto æ–‡ä»¶
"""

import json
from fastapi import APIRouter, HTTPException, Request
from app.models import MessageType, ChatRequest
from app.database import sessions_rls_repo, chats_rls_repo
from app.utils.sse import create_sse_response
from app.config import settings
from app.utils import log_info, log_error, log_debug

router = APIRouter()


class StageResponseProcessor:
    """ä¸ºæ¯ä¸ª stage å¤„ç†å“åº”çš„å¤„ç†å™¨ç±»"""
    
    @staticmethod
    def process_clarification_response(agent_state: dict) -> dict:
        """å¤„ç†æ¾„æ¸…é˜¶æ®µå“åº”"""
        conversations = agent_state.get("conversations", [])
        if conversations:
            latest_message = conversations[-1].get("text", "")
            return {
                "type": "ai_message",
                "content": {
                    "text": latest_message,
                    "role": "assistant",
                    "stage": "clarification"
                }
            }
        return {
            "type": "ai_message", 
            "content": {
                "text": "è¯·æè¿°æ‚¨æƒ³è¦åˆ›å»ºçš„å·¥ä½œæµ",
                "role": "assistant",
                "stage": "clarification"
            }
        }
    
    @staticmethod
    def process_negotiation_response(agent_state: dict) -> dict:
        """å¤„ç†åå•†é˜¶æ®µå“åº”"""
        conversations = agent_state.get("conversations", [])
        clarification_context = agent_state.get("clarification_context", {})
        pending_questions = clarification_context.get("pending_questions", [])
        
        if conversations:
            latest_message = conversations[-1].get("text", "")
            response = {
                "type": "ai_message",
                "content": {
                    "text": latest_message,
                    "role": "assistant",
                    "stage": "negotiation"
                }
            }
            if pending_questions:
                response["content"]["questions"] = pending_questions
            return response
        
        return {
            "type": "ai_message",
            "content": {
                "text": "è¯·æä¾›æ›´å¤šä¿¡æ¯ä»¥ä¾¿æˆ‘æ›´å¥½åœ°ç†è§£æ‚¨çš„éœ€æ±‚",
                "role": "assistant",
                "stage": "negotiation"
            }
        }
    
    @staticmethod
    def process_gap_analysis_response(agent_state: dict) -> dict:
        """å¤„ç†å·®è·åˆ†æé˜¶æ®µå“åº”"""
        gaps = agent_state.get("gaps", [])
        return {
            "type": "ai_message",
            "content": {
                "text": f"éœ€æ±‚åˆ†æå®Œæˆï¼Œè¯†åˆ«äº† {len(gaps)} ä¸ªæŠ€æœ¯èƒ½åŠ›ç¼ºå£" if gaps else "éœ€æ±‚åˆ†æå®Œæˆï¼Œå¯ä»¥ç›´æ¥å®ç°",
                "role": "assistant",
                "stage": "gap_analysis",
                "gaps": gaps
            }
        }
    
    @staticmethod
    def process_alternative_generation_response(agent_state: dict) -> dict:
        """å¤„ç†æ›¿ä»£æ–¹æ¡ˆç”Ÿæˆé˜¶æ®µå“åº” - è¿”å› alternatives æ•°æ®"""
        alternatives = agent_state.get("alternatives", [])
        conversations = agent_state.get("conversations", [])
        
        latest_message = ""
        if conversations:
            latest_message = conversations[-1].get("text", "")
        
        # è¿™é‡Œè¿”å› alternatives ç±»å‹è€Œä¸æ˜¯ ai_message
        return {
            "type": "alternatives",
            "content": {
                "text": latest_message or "åŸºäºæŠ€æœ¯èƒ½åŠ›åˆ†æï¼Œä¸ºæ‚¨æä¾›ä»¥ä¸‹æ›¿ä»£æ–¹æ¡ˆï¼š",
                "role": "assistant",
                "stage": "alternative_generation",
                "alternatives": alternatives
            }
        }
    
    @staticmethod
    def process_workflow_generation_response(agent_state: dict) -> dict:
        """å¤„ç†å·¥ä½œæµç”Ÿæˆé˜¶æ®µå“åº” - è¿”å› workflow æ•°æ®"""
        current_workflow_json = agent_state.get("current_workflow_json", "")
        
        try:
            workflow_data = json.loads(current_workflow_json) if current_workflow_json else {}
        except json.JSONDecodeError:
            workflow_data = {}
        
        # è¿™é‡Œè¿”å› workflow ç±»å‹
        return {
            "type": "workflow",
            "content": {
                "text": "å·¥ä½œæµç”Ÿæˆå®Œæˆ",
                "role": "assistant",
                "stage": "workflow_generation"
            },
            "workflow": workflow_data
        }
    
    @staticmethod
    def process_debug_response(agent_state: dict) -> dict:
        """å¤„ç†è°ƒè¯•é˜¶æ®µå“åº”"""
        debug_result = agent_state.get("debug_result", "")
        
        try:
            debug_data = json.loads(debug_result) if debug_result else {}
        except json.JSONDecodeError:
            debug_data = {"success": False, "errors": ["Invalid debug result"]}
        
        return {
            "type": "ai_message",
            "content": {
                "text": "å·¥ä½œæµéªŒè¯å®Œæˆ" if debug_data.get("success") else "å·¥ä½œæµéªŒè¯å‘ç°é—®é¢˜",
                "role": "assistant",
                "stage": "debug",
                "debug_info": debug_data
            }
        }
    
    @staticmethod
    def process_completed_response(agent_state: dict) -> dict:
        """å¤„ç†å®Œæˆé˜¶æ®µå“åº” - è¿”å›æœ€ç»ˆ workflow"""
        current_workflow_json = agent_state.get("current_workflow_json", "")
        
        try:
            workflow_data = json.loads(current_workflow_json) if current_workflow_json else {}
        except json.JSONDecodeError:
            workflow_data = {}
        
        return {
            "type": "workflow",
            "content": {
                "text": "å·¥ä½œæµåˆ›å»ºå®Œæˆï¼",
                "role": "assistant",
                "stage": "completed"
            },
            "workflow": workflow_data
        }
    
    @classmethod
    def process_stage_response(cls, stage: str, agent_state: dict) -> dict:
        """æ ¹æ® stage å¤„ç†å“åº”"""
        processors = {
            "clarification": cls.process_clarification_response,
            "negotiation": cls.process_negotiation_response,
            "gap_analysis": cls.process_gap_analysis_response,
            "alternative_generation": cls.process_alternative_generation_response,
            "workflow_generation": cls.process_workflow_generation_response,
            "debug": cls.process_debug_response,
            "completed": cls.process_completed_response
        }
        
        processor = processors.get(stage, cls.process_clarification_response)
        return processor(agent_state)


@router.post("/chat/stream")
async def chat_stream(
    chat_request: ChatRequest,
    http_request: Request
):
    """
    æ–°çš„æµå¼å¯¹è¯æ¥å£ - æ”¯æŒä¸‰ç§è¿”å›ç±»å‹
    
    è¿”å›ç±»å‹ï¼š
    1. ai_message - çº¯æ–‡æœ¬æ¶ˆæ¯ (clarification, negotiation, gap_analysis, debug)
    2. workflow - å·¥ä½œæµæ•°æ® (workflow_generation, completed) 
    3. error - é”™è¯¯æ¶ˆæ¯
    
    åªåœ¨ is_final=true æ—¶ä¿å­˜çŠ¶æ€åˆ°æ•°æ®åº“
    """
    try:
        # è·å–ç”¨æˆ·å’Œè®¿é—®ä»¤ç‰Œ
        user = getattr(http_request.state, 'user', None)
        user_id = user.get("sub") if user else "anonymous"
        access_token = getattr(http_request.state, 'access_token', None)
        
        # éªŒè¯ä¼šè¯
        session = sessions_rls_repo.get_by_id(chat_request.session_id, access_token=access_token)
        if not session:
            raise HTTPException(status_code=404, detail="Session not found")
        
        # å­˜å‚¨ç”¨æˆ·æ¶ˆæ¯
        user_message_data = {
            "session_id": chat_request.session_id,
            "user_id": user_id,
            "message_type": MessageType.USER.value,
            "content": chat_request.message
        }
        
        stored_user_message = chats_rls_repo.create(user_message_data)
        if not stored_user_message:
            raise HTTPException(status_code=500, detail="Failed to store user message")
        
        # å¯¼å…¥ gRPC å®¢æˆ·ç«¯
        from app.services.grpc_client import workflow_client
        from app.services.state_manager import get_state_manager
        
        # åˆ›å»ºæµå¼å“åº”
        async def enhanced_workflow_stream():
            state_manager = get_state_manager()
            
            # å‡†å¤‡å·¥ä½œæµä¸Šä¸‹æ–‡
            action_type = session.get("action_type", "create")
            source_workflow_id = session.get("source_workflow_id", "")
            
            workflow_context = {
                "origin": action_type,
                "source_workflow_id": source_workflow_id,
                "modification_intent": f"User requested {action_type} workflow"
            }
            
            # åˆ›å»ºæˆ–è·å–å·¥ä½œæµçŠ¶æ€
            existing_state = state_manager.get_state_by_session(chat_request.session_id, access_token)
            if not existing_state:
                state_id = state_manager.create_state(
                    session_id=chat_request.session_id,
                    user_id=user_id,
                    clarification_context={"origin": action_type, "pending_questions": []},
                    workflow_context=workflow_context,
                    access_token=access_token
                )
                log_debug(f"Created workflow state for session {chat_request.session_id}")
            
            # å¤„ç†å¯¹è¯æµ
            final_ai_response = ""
            final_workflow_data = None
            
            async for response in workflow_client.process_conversation_stream(
                session_id=chat_request.session_id,
                user_message=chat_request.message,
                user_id=user_id,
                workflow_context=workflow_context,
                access_token=access_token
            ):
                log_debug(f"Received response type: {response.get('type')}")
                
                # å¤„ç†é”™è¯¯å“åº”
                if response["type"] == "error":
                    yield {
                        "type": "error",
                        "session_id": response["session_id"],
                        "timestamp": response["timestamp"],
                        "is_final": response.get("is_final", True),
                        "content": response["error"]
                    }
                    return
                
                # å¤„ç†æ­£å¸¸å“åº”
                if response["type"] == "message" and "agent_state" in response:
                    agent_state = response["agent_state"]
                    stage = agent_state.get("stage", "clarification")
                    
                    # ä½¿ç”¨ StageResponseProcessor å¤„ç†å“åº”
                    processed_response = StageResponseProcessor.process_stage_response(stage, agent_state)
                    
                    # æ„å»º SSE å“åº”
                    sse_data = {
                        "type": processed_response["type"],
                        "session_id": response["session_id"],
                        "timestamp": response["timestamp"],
                        "is_final": response.get("is_final", False),
                        "content": processed_response["content"]
                    }
                    
                    # æ·»åŠ  workflow æ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
                    if "workflow" in processed_response:
                        sse_data["workflow"] = processed_response["workflow"]
                        final_workflow_data = processed_response["workflow"]
                    
                    # æ”¶é›† AI å“åº”æ–‡æœ¬
                    if processed_response["type"] in ["ai_message", "alternatives"]:
                        final_ai_response += processed_response["content"].get("text", "")
                    
                    yield sse_data
                    
                    # å¦‚æœæ˜¯æœ€ç»ˆå“åº”ï¼Œé€€å‡ºå¾ªç¯
                    if response.get("is_final", False):
                        break
                else:
                    # å¤„ç†çŠ¶æ€æ›´æ–°ç­‰å…¶ä»–å“åº”
                    yield {
                        "type": "status",
                        "session_id": response["session_id"],
                        "timestamp": response["timestamp"],
                        "is_final": response.get("is_final", False),
                        "content": {"message": "Processing..."}
                    }
            
            # å­˜å‚¨æœ€ç»ˆ AI å“åº”åˆ°æ•°æ®åº“
            if final_ai_response.strip():
                ai_message_data = {
                    "session_id": chat_request.session_id,
                    "user_id": user_id,
                    "message_type": MessageType.ASSISTANT.value,
                    "content": final_ai_response
                }
                
                ai_message = chats_rls_repo.create(ai_message_data)
                if settings.DEBUG and ai_message:
                    log_info(f"ğŸ“ Stored AI response: {ai_message['id']}")
        
        return create_sse_response(enhanced_workflow_stream())
    
    except HTTPException:
        raise
    except Exception as e:
        log_error(f"Error in chat stream: {e}")
        raise HTTPException(status_code=500, detail="Internal server error")


@router.get("/chat/{session_id}/messages")
async def get_chat_history(session_id: str, http_request: Request):
    """
    è·å–èŠå¤©å†å²è®°å½•
    """
    try:
        # è·å–è®¿é—®ä»¤ç‰Œ
        access_token = getattr(http_request.state, 'access_token', None)
        
        # éªŒè¯ä¼šè¯å­˜åœ¨
        session = sessions_rls_repo.get_by_id(session_id, access_token=access_token)
        if not session:
            raise HTTPException(status_code=404, detail="Session not found")
        
        # è·å–æ‰€æœ‰æ¶ˆæ¯
        messages = chats_rls_repo.get_by_session_id(session_id, access_token=access_token)
        
        # æŒ‰åºåˆ—å·æ’åº
        messages.sort(key=lambda x: x["sequence_number"])
        
        return {
            "session_id": session_id,
            "messages": messages,
            "total_count": len(messages)
        }
    
    except HTTPException:
        raise
    except Exception as e:
        if settings.DEBUG:
            log_error(f"Error getting chat history: {e}")
        raise HTTPException(status_code=500, detail="Internal server error")