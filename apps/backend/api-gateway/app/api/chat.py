"""
æ–°çš„ Chat API - æ”¯æŒä¸‰ç§è¿”å›ç±»å‹ï¼šai message, workflow, error
åŸºäºæ–°çš„ workflow_agent.proto æ–‡ä»¶
"""

import json
import structlog
from fastapi import APIRouter, HTTPException, Request
from app.models import MessageType, ChatRequest
from app.database import sessions_rls_repo, chats_rls_repo
from app.utils.sse import create_sse_response
from app.config import settings

logger = structlog.get_logger("chat_api")

router = APIRouter()


# ç§»é™¤é‡å¤çš„ StageResponseProcessorï¼Œä½¿ç”¨ç»Ÿä¸€çš„ UnifiedResponseProcessor


@router.post("/chat/stream")
async def chat_stream(
    chat_request: ChatRequest,
    http_request: Request
):
    """
    æ–°çš„æµå¼å¯¹è¯æ¥å£ - æ”¯æŒä¸‰ç§è¿”å›ç±»å‹
    
    è¿”å›ç±»å‹ï¼š
    1. ai_message - çº¯æ–‡æœ¬æ¶ˆæ¯ (clarification, negotiation, gap_analysis, debug)
    2. workflow - å·¥ä½œæµæ•°æ® (workflow_generation, completed) 
    3. error - é”™è¯¯æ¶ˆæ¯
    
    åªåœ¨ is_final=true æ—¶ä¿å­˜çŠ¶æ€åˆ°æ•°æ®åº“
    """
    try:
        # è·å–ç”¨æˆ·å’Œè®¿é—®ä»¤ç‰Œ
        user = getattr(http_request.state, 'user', None)
        user_id = user.get("sub") if user else "anonymous"
        access_token = getattr(http_request.state, 'access_token', None)
        
        # éªŒè¯ä¼šè¯
        session = sessions_rls_repo.get_by_id(chat_request.session_id, access_token=access_token)
        if not session:
            raise HTTPException(status_code=404, detail="Session not found")
        
        # å­˜å‚¨ç”¨æˆ·æ¶ˆæ¯
        user_message_data = {
            "session_id": chat_request.session_id,
            "user_id": user_id,
            "message_type": MessageType.USER.value,
            "content": chat_request.message
        }
        
        stored_user_message = chats_rls_repo.create(user_message_data)
        if not stored_user_message:
            raise HTTPException(status_code=500, detail="Failed to store user message")
        
        # å¯¼å…¥ gRPC å®¢æˆ·ç«¯
        from app.services.grpc_client import workflow_client
        from app.services.state_manager import get_state_manager
        
        # åˆ›å»ºæµå¼å“åº”
        async def enhanced_workflow_stream():
            state_manager = get_state_manager()
            
            # å‡†å¤‡å·¥ä½œæµä¸Šä¸‹æ–‡
            action_type = session.get("action_type", "create")
            source_workflow_id = session.get("source_workflow_id", "")
            
            workflow_context = {
                "origin": action_type,
                "source_workflow_id": source_workflow_id,
                "modification_intent": f"User requested {action_type} workflow"
            }
            
            # åˆ›å»ºæˆ–è·å–å·¥ä½œæµçŠ¶æ€
            existing_state = state_manager.get_state_by_session(chat_request.session_id, access_token)
            if not existing_state:
                state_id = state_manager.create_state(
                    session_id=chat_request.session_id,
                    user_id=user_id,
                    clarification_context={"origin": action_type, "pending_questions": []},
                    workflow_context=workflow_context,
                    access_token=access_token
                )
                logger.debug("Created workflow state for session", session_id=chat_request.session_id)
            
            # å¤„ç†å¯¹è¯æµ
            final_ai_response = ""
            final_workflow_data = None
            
            async for response in workflow_client.process_conversation_stream(
                session_id=chat_request.session_id,
                user_message=chat_request.message,
                user_id=user_id,
                workflow_context=workflow_context,
                access_token=access_token
            ):
                logger.debug("Received response type", response_type=response.get('type'))
                
                # å¤„ç†é”™è¯¯å“åº”
                if response["type"] == "error":
                    yield {
                        "type": "error",
                        "session_id": response["session_id"],
                        "timestamp": response["timestamp"],
                        "is_final": response.get("is_final", True),
                        "content": response["error"]
                    }
                    return
                
                # å¤„ç†æ­£å¸¸å“åº”ï¼ˆgrpc_clientå·²ç»å¤„ç†è¿‡ç±»å‹è½¬æ¢ï¼‰
                if response["type"] in ["ai_message", "workflow", "alternatives"] and "agent_state" in response:
                    # grpc_client å·²ç»è°ƒç”¨è¿‡ UnifiedResponseProcessorï¼Œç›´æ¥ä½¿ç”¨å¤„ç†ç»“æœ
                    # æ„å»º SSE å“åº”
                    sse_data = {
                        "type": response["type"],
                        "session_id": response["session_id"],
                        "timestamp": response["timestamp"],
                        "is_final": response.get("is_final", False),
                        "content": response["content"]
                    }
                    
                    # æ·»åŠ  workflow æ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
                    if "workflow" in response:
                        sse_data["workflow"] = response["workflow"]
                        final_workflow_data = response["workflow"]
                    
                    # æ”¶é›† AI å“åº”æ–‡æœ¬
                    if response["type"] in ["ai_message", "alternatives"]:
                        final_ai_response += response["content"].get("text", "")
                    
                    yield sse_data
                    
                    # å¦‚æœæ˜¯æœ€ç»ˆå“åº”ï¼Œé€€å‡ºå¾ªç¯
                    if response.get("is_final", False):
                        break
                else:
                    # å¤„ç†çŠ¶æ€æ›´æ–°ç­‰å…¶ä»–å“åº”
                    yield {
                        "type": "status",
                        "session_id": response["session_id"],
                        "timestamp": response["timestamp"],
                        "is_final": response.get("is_final", False),
                        "content": {"message": "Processing..."}
                    }
            
            # å­˜å‚¨æœ€ç»ˆ AI å“åº”åˆ°æ•°æ®åº“
            if final_ai_response.strip():
                ai_message_data = {
                    "session_id": chat_request.session_id,
                    "user_id": user_id,
                    "message_type": MessageType.ASSISTANT.value,
                    "content": final_ai_response
                }
                
                ai_message = chats_rls_repo.create(ai_message_data)
                if settings.DEBUG and ai_message:
                    logger.info("ğŸ“ Stored AI response", message_id=ai_message['id'])
        
        return create_sse_response(enhanced_workflow_stream())
    
    except HTTPException:
        raise
    except Exception as e:
        logger.exception("Error in chat stream", error=str(e))
        raise HTTPException(status_code=500, detail="Internal server error")


@router.get("/chat/{session_id}/messages")
async def get_chat_history(session_id: str, http_request: Request):
    """
    è·å–èŠå¤©å†å²è®°å½•
    """
    try:
        # è·å–è®¿é—®ä»¤ç‰Œ
        access_token = getattr(http_request.state, 'access_token', None)
        
        # éªŒè¯ä¼šè¯å­˜åœ¨
        session = sessions_rls_repo.get_by_id(session_id, access_token=access_token)
        if not session:
            raise HTTPException(status_code=404, detail="Session not found")
        
        # è·å–æ‰€æœ‰æ¶ˆæ¯
        messages = chats_rls_repo.get_by_session_id(session_id, access_token=access_token)
        
        # æŒ‰åºåˆ—å·æ’åº
        messages.sort(key=lambda x: x["sequence_number"])
        
        return {
            "session_id": session_id,
            "messages": messages,
            "total_count": len(messages)
        }
    
    except HTTPException:
        raise
    except Exception as e:
        if settings.DEBUG:
            logger.exception("Error getting chat history", error=str(e))
        raise HTTPException(status_code=500, detail="Internal server error")