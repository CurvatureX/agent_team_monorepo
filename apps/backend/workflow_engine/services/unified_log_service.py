"""
ç»Ÿä¸€æ‰§è¡Œæ—¥å¿—æœåŠ¡
æ”¯æŒæŠ€æœ¯è°ƒè¯•æ—¥å¿—å’Œç”¨æˆ·å‹å¥½ä¸šåŠ¡æ—¥å¿—
"""

import asyncio
import base64
import json
import logging

# Import database models
import sys
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Union

backend_dir = Path(__file__).parent.parent.parent.parent
sys.path.insert(0, str(backend_dir))

try:
    from models.database import get_db_session
    from shared.models.db_models import (
        DisplayPriorityEnum,
        LogCategoryEnum,
        LogEventTypeEnum,
        LogLevelEnum,
        WorkflowExecutionLog,
    )
    from sqlalchemy.orm import Session

    DATABASE_AVAILABLE = True
except ImportError:
    DATABASE_AVAILABLE = False
    WorkflowExecutionLog = None
    LogLevelEnum = None
    LogEventTypeEnum = None
    LogCategoryEnum = None
    DisplayPriorityEnum = None

try:
    import redis

    REDIS_AVAILABLE = True
except ImportError:
    REDIS_AVAILABLE = False


@dataclass
class PaginationResult:
    """åˆ†é¡µæŸ¥è¯¢ç»“æœ"""

    data: List[Dict[str, Any]]
    total_count: int
    page: int
    page_size: int
    has_next: bool
    has_previous: bool
    next_cursor: Optional[str] = None
    previous_cursor: Optional[str] = None


@dataclass
class UnifiedLogEntry:
    """ç»Ÿä¸€æ—¥å¿—æ¡ç›®"""

    execution_id: str
    log_category: str  # LogCategoryEnum
    event_type: str  # LogEventTypeEnum
    level: str  # LogLevelEnum
    message: str
    timestamp: Optional[str] = None
    user_friendly_message: Optional[str] = None
    display_priority: int = 5
    is_milestone: bool = False

    # èŠ‚ç‚¹ä¿¡æ¯
    node_id: Optional[str] = None
    node_name: Optional[str] = None
    node_type: Optional[str] = None

    # è¿›åº¦ä¿¡æ¯
    step_number: Optional[int] = None
    total_steps: Optional[int] = None
    progress_percentage: Optional[float] = None
    duration_seconds: Optional[int] = None

    # ç»“æ„åŒ–æ•°æ®
    data: Optional[Dict[str, Any]] = None
    technical_details: Optional[Dict[str, Any]] = None
    stack_trace: Optional[str] = None
    performance_metrics: Optional[Dict[str, Any]] = None

    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.now().isoformat()
        if self.data is None:
            self.data = {}
        if self.technical_details is None:
            self.technical_details = {}
        if self.performance_metrics is None:
            self.performance_metrics = {}

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            "execution_id": self.execution_id,
            "log_category": self.log_category,
            "event_type": self.event_type,
            "level": self.level,
            "message": self.message,
            "user_friendly_message": self.user_friendly_message,
            "display_priority": self.display_priority,
            "is_milestone": self.is_milestone,
            "timestamp": self.timestamp,
            "node_id": self.node_id,
            "node_name": self.node_name,
            "node_type": self.node_type,
            "step_number": self.step_number,
            "total_steps": self.total_steps,
            "progress_percentage": self.progress_percentage,
            "duration_seconds": self.duration_seconds,
            "data": self.data,
            "technical_details": self.technical_details,
            "stack_trace": self.stack_trace,
            "performance_metrics": self.performance_metrics,
        }


class UnifiedExecutionLogService:
    """ç»Ÿä¸€æ‰§è¡Œæ—¥å¿—æœåŠ¡ - æ”¯æŒæŠ€æœ¯å’Œä¸šåŠ¡æ—¥å¿—"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)

        # Redisè¿æ¥
        self.redis_client = None
        if REDIS_AVAILABLE:
            try:
                self.redis_client = redis.Redis(
                    host="localhost", port=6379, db=1, decode_responses=True
                )
                self.redis_client.ping()
                self.logger.info("Redis connection established for unified log service")
            except Exception as e:
                self.logger.warning(f"Redis connection failed: {e}")
                self.redis_client = None

        # å†…å­˜ç¼“å­˜
        self._memory_cache: Dict[str, List[UnifiedLogEntry]] = {}

        # WebSocketè¿æ¥ç®¡ç†
        self._websocket_connections: Dict[str, List[Any]] = {}

    async def add_technical_log(
        self,
        execution_id: str,
        level: str,
        message: str,
        event_type: str,
        node_id: Optional[str] = None,
        node_name: Optional[str] = None,
        node_type: Optional[str] = None,
        technical_details: Optional[Dict[str, Any]] = None,
        stack_trace: Optional[str] = None,
        performance_metrics: Optional[Dict[str, Any]] = None,
        duration_seconds: Optional[int] = None,
        data: Optional[Dict[str, Any]] = None,
    ):
        """æ·»åŠ æŠ€æœ¯è°ƒè¯•æ—¥å¿—"""
        log_entry = UnifiedLogEntry(
            execution_id=execution_id,
            log_category=LogCategoryEnum.TECHNICAL.value if LogCategoryEnum else "technical",
            level=level,
            message=message,
            event_type=event_type,
            node_id=node_id,
            node_name=node_name,
            node_type=node_type,
            technical_details=technical_details,
            stack_trace=stack_trace,
            performance_metrics=performance_metrics,
            duration_seconds=duration_seconds,
            data=data,
            display_priority=DisplayPriorityEnum.LOW.value
            if DisplayPriorityEnum
            else 3,  # æŠ€æœ¯æ—¥å¿—é€šå¸¸ä¼˜å…ˆçº§è¾ƒä½
        )
        await self._store_log_entry(log_entry)

    async def add_business_log(
        self,
        execution_id: str,
        event_type: str,
        technical_message: str,
        user_friendly_message: str,
        level: str = "INFO",
        display_priority: Optional[int] = None,
        is_milestone: bool = False,
        step_number: Optional[int] = None,
        total_steps: Optional[int] = None,
        progress_percentage: Optional[float] = None,
        duration_seconds: Optional[int] = None,
        node_id: Optional[str] = None,
        node_name: Optional[str] = None,
        node_type: Optional[str] = None,
        data: Optional[Dict[str, Any]] = None,
    ):
        """æ·»åŠ ä¸šåŠ¡å‹å¥½æ—¥å¿—"""
        if display_priority is None:
            display_priority = DisplayPriorityEnum.NORMAL.value if DisplayPriorityEnum else 5

        log_entry = UnifiedLogEntry(
            execution_id=execution_id,
            log_category=LogCategoryEnum.BUSINESS.value if LogCategoryEnum else "business",
            level=level,
            message=technical_message,
            user_friendly_message=user_friendly_message,
            event_type=event_type,
            display_priority=display_priority,
            is_milestone=is_milestone,
            step_number=step_number,
            total_steps=total_steps,
            progress_percentage=progress_percentage,
            duration_seconds=duration_seconds,
            node_id=node_id,
            node_name=node_name,
            node_type=node_type,
            data=data,
        )
        await self._store_log_entry(log_entry)

    async def _store_log_entry(self, entry: UnifiedLogEntry):
        """å­˜å‚¨æ—¥å¿—æ¡ç›®"""
        try:
            # 1. å­˜å‚¨åˆ°Redisç¼“å­˜
            if self.redis_client:
                await self._store_to_redis(entry)
            else:
                await self._store_to_memory(entry)

            # 2. æ¨é€åˆ°WebSocketè¿æ¥
            await self._push_to_websockets(entry)

            # 3. å¼‚æ­¥å­˜å‚¨åˆ°æ•°æ®åº“
            asyncio.create_task(self._store_to_database(entry))

        except Exception as e:
            self.logger.error(f"Failed to store log entry: {e}")

    async def _store_to_redis(self, entry: UnifiedLogEntry):
        """å­˜å‚¨åˆ°Redis"""
        try:
            # åˆ†ç±»å­˜å‚¨ - æŠ€æœ¯æ—¥å¿—å’Œä¸šåŠ¡æ—¥å¿—åˆ†å¼€ç¼“å­˜
            tech_key = f"workflow_logs:technical:{entry.execution_id}"
            business_key = f"workflow_logs:business:{entry.execution_id}"

            value = json.dumps(entry.to_dict(), ensure_ascii=False)

            if entry.log_category == (
                LogCategoryEnum.TECHNICAL.value if LogCategoryEnum else "technical"
            ):
                self.redis_client.lpush(tech_key, value)
                self.redis_client.expire(tech_key, 24 * 3600)  # 24å°æ—¶è¿‡æœŸ
            else:
                self.redis_client.lpush(business_key, value)
                self.redis_client.expire(business_key, 7 * 24 * 3600)  # ä¸šåŠ¡æ—¥å¿—ä¿ç•™7å¤©

            # å…¨å±€æœ€è¿‘æ—¥å¿—
            global_key = f"recent_logs:{entry.log_category}"
            self.redis_client.lpush(global_key, value)
            self.redis_client.ltrim(global_key, 0, 999)  # ä¿ç•™æœ€è¿‘1000æ¡

        except Exception as e:
            self.logger.error(f"Failed to store to Redis: {e}")
            await self._store_to_memory(entry)

    async def _store_to_memory(self, entry: UnifiedLogEntry):
        """å­˜å‚¨åˆ°å†…å­˜ç¼“å­˜"""
        cache_key = f"{entry.execution_id}:{entry.log_category}"
        if cache_key not in self._memory_cache:
            self._memory_cache[cache_key] = []

        self._memory_cache[cache_key].append(entry)

        # é™åˆ¶å†…å­˜ç¼“å­˜å¤§å°
        if len(self._memory_cache[cache_key]) > 500:
            self._memory_cache[cache_key] = self._memory_cache[cache_key][-200:]

    async def _store_to_database(self, entry: UnifiedLogEntry):
        """å­˜å‚¨åˆ°æ•°æ®åº“"""
        if not DATABASE_AVAILABLE:
            return

        try:
            db = get_db_session()
            try:
                db_log = WorkflowExecutionLog(
                    execution_id=entry.execution_id,
                    log_category=LogCategoryEnum(entry.log_category),
                    event_type=entry.event_type,  # Pass string directly, SQLAlchemy will handle enum conversion
                    level=LogLevelEnum(entry.level),
                    message=entry.message,
                    user_friendly_message=entry.user_friendly_message,
                    display_priority=entry.display_priority,
                    is_milestone=entry.is_milestone,
                    data=entry.data or {},
                    node_id=entry.node_id,
                    node_name=entry.node_name,
                    node_type=entry.node_type,
                    step_number=entry.step_number,
                    total_steps=entry.total_steps,
                    progress_percentage=entry.progress_percentage,
                    duration_seconds=entry.duration_seconds,
                    technical_details=entry.technical_details or {},
                    stack_trace=entry.stack_trace,
                    performance_metrics=entry.performance_metrics or {},
                )

                db.add(db_log)
                db.commit()

            finally:
                db.close()

        except Exception as e:
            self.logger.error(f"Failed to store to database: {e}")

    async def _push_to_websockets(self, entry: UnifiedLogEntry):
        """æ¨é€åˆ°WebSocketè¿æ¥"""
        execution_id = entry.execution_id
        if execution_id in self._websocket_connections:
            active_connections = []
            for websocket in self._websocket_connections[execution_id]:
                try:
                    await websocket.send_text(json.dumps(entry.to_dict(), ensure_ascii=False))
                    active_connections.append(websocket)
                except Exception:
                    pass

            self._websocket_connections[execution_id] = active_connections
            if not active_connections:
                del self._websocket_connections[execution_id]

    async def get_business_logs(
        self,
        execution_id: str,
        min_priority: int = 5,
        milestones_only: bool = False,
        limit: int = 100,
        page: int = 1,
        cursor: Optional[str] = None,
    ) -> PaginationResult:
        """è·å–ä¸šåŠ¡å‹å¥½æ—¥å¿— - å‰ç«¯ç”¨æˆ·ç•Œé¢ï¼ˆæ”¯æŒåˆ†é¡µï¼‰"""
        # éªŒè¯å‚æ•°
        limit = max(1, min(limit, 100))  # é™åˆ¶æ¯é¡µæœ€å¤š100æ¡
        page = max(1, page)

        try:
            # ä¼˜å…ˆä»æ•°æ®åº“è·å–ï¼ˆæ”¯æŒæ›´å‡†ç¡®çš„åˆ†é¡µï¼‰
            if DATABASE_AVAILABLE:
                return await self._get_business_logs_from_database_paginated(
                    execution_id, min_priority, milestones_only, limit, page, cursor
                )

            # å¤‡é€‰ï¼šä»Redisè·å–ï¼ˆè¾ƒç®€å•çš„åˆ†é¡µæ”¯æŒï¼‰
            if self.redis_client:
                return await self._get_business_logs_from_redis_paginated(
                    execution_id, min_priority, milestones_only, limit, page
                )

            return PaginationResult(
                data=[],
                total_count=0,
                page=page,
                page_size=limit,
                has_next=False,
                has_previous=False,
            )

        except Exception as e:
            self.logger.error(f"Failed to get business logs: {e}")
            return PaginationResult(
                data=[],
                total_count=0,
                page=page,
                page_size=limit,
                has_next=False,
                has_previous=False,
            )

    async def get_technical_logs(
        self,
        execution_id: str,
        level: Optional[str] = None,
        limit: int = 100,
        page: int = 1,
        cursor: Optional[str] = None,
    ) -> PaginationResult:
        """è·å–æŠ€æœ¯è°ƒè¯•æ—¥å¿— - AI Agentåˆ†æï¼ˆæ”¯æŒåˆ†é¡µï¼‰"""
        # éªŒè¯å‚æ•°
        limit = max(1, min(limit, 100))
        page = max(1, page)

        try:
            # ä¼˜å…ˆä»æ•°æ®åº“è·å–ï¼ˆæ”¯æŒæ›´å‡†ç¡®çš„åˆ†é¡µï¼‰
            if DATABASE_AVAILABLE:
                return await self._get_technical_logs_from_database_paginated(
                    execution_id, level, limit, page, cursor
                )

            # å¤‡é€‰ï¼šä»Redisè·å–
            if self.redis_client:
                return await self._get_technical_logs_from_redis_paginated(
                    execution_id, level, limit, page
                )

            return PaginationResult(
                data=[],
                total_count=0,
                page=page,
                page_size=limit,
                has_next=False,
                has_previous=False,
            )

        except Exception as e:
            self.logger.error(f"Failed to get technical logs: {e}")
            return PaginationResult(
                data=[],
                total_count=0,
                page=page,
                page_size=limit,
                has_next=False,
                has_previous=False,
            )

    async def _get_business_logs_from_redis(
        self, execution_id: str, min_priority: int, milestones_only: bool, limit: int, offset: int
    ) -> List[Dict[str, Any]]:
        """ä»Redisè·å–ä¸šåŠ¡æ—¥å¿—"""
        try:
            key = f"workflow_logs:business:{execution_id}"
            raw_logs = self.redis_client.lrange(key, 0, -1)

            logs = []
            for raw_log in reversed(raw_logs):
                log = json.loads(raw_log)

                # åº”ç”¨è¿‡æ»¤æ¡ä»¶
                if log.get("display_priority", 5) < min_priority:
                    continue
                if milestones_only and not log.get("is_milestone", False):
                    continue

                logs.append(log)

            # åº”ç”¨åˆ†é¡µ
            return logs[offset : offset + limit]

        except Exception as e:
            self.logger.error(f"Failed to get business logs from Redis: {e}")
            return []

    async def _get_business_logs_from_database(
        self, execution_id: str, min_priority: int, milestones_only: bool, limit: int, offset: int
    ) -> List[Dict[str, Any]]:
        """ä»æ•°æ®åº“è·å–ä¸šåŠ¡æ—¥å¿—"""
        try:
            db = get_db_session()
            try:
                query = db.query(WorkflowExecutionLog).filter(
                    WorkflowExecutionLog.execution_id == execution_id,
                    WorkflowExecutionLog.log_category == LogCategoryEnum.BUSINESS,
                    WorkflowExecutionLog.display_priority >= min_priority,
                )

                if milestones_only:
                    query = query.filter(WorkflowExecutionLog.is_milestone == True)

                results = (
                    query.order_by(WorkflowExecutionLog.created_at.asc())
                    .offset(offset)
                    .limit(limit)
                    .all()
                )

                logs = []
                for log_entry in results:
                    log_dict = log_entry.to_dict()
                    # ä¼˜å…ˆæ˜¾ç¤ºç”¨æˆ·å‹å¥½æ¶ˆæ¯
                    log_dict["display_message"] = log_entry.display_message
                    logs.append(log_dict)

                return logs

            finally:
                db.close()

        except Exception as e:
            self.logger.error(f"Failed to get business logs from database: {e}")
            return []

    async def _get_business_logs_from_database_paginated(
        self,
        execution_id: str,
        min_priority: int,
        milestones_only: bool,
        limit: int,
        page: int,
        cursor: Optional[str] = None,
    ) -> PaginationResult:
        """ä»æ•°æ®åº“è·å–ä¸šåŠ¡æ—¥å¿—ï¼ˆåˆ†é¡µï¼‰"""
        try:
            db = get_db_session()
            try:
                # æ„å»ºåŸºç¡€æŸ¥è¯¢
                base_query = db.query(WorkflowExecutionLog).filter(
                    WorkflowExecutionLog.execution_id == execution_id,
                    WorkflowExecutionLog.log_category == LogCategoryEnum.BUSINESS,
                    WorkflowExecutionLog.display_priority >= min_priority,
                )

                if milestones_only:
                    base_query = base_query.filter(WorkflowExecutionLog.is_milestone == True)

                # è·å–æ€»æ•°é‡
                total_count = base_query.count()

                # å¤„ç†æ¸¸æ ‡åˆ†é¡µ
                if cursor:
                    try:
                        cursor_data = json.loads(base64.b64decode(cursor).decode("utf-8"))
                        cursor_timestamp = cursor_data.get("timestamp")
                        cursor_id = cursor_data.get("id")

                        if cursor_timestamp and cursor_id:
                            # æ¸¸æ ‡åˆ†é¡µæŸ¥è¯¢
                            base_query = base_query.filter(
                                (WorkflowExecutionLog.created_at > cursor_timestamp)
                                | (
                                    (WorkflowExecutionLog.created_at == cursor_timestamp)
                                    & (WorkflowExecutionLog.id > cursor_id)
                                )
                            )
                    except Exception as e:
                        self.logger.warning(
                            f"Invalid cursor, falling back to offset pagination: {e}"
                        )
                        cursor = None

                # å¦‚æœæ²¡æœ‰æœ‰æ•ˆæ¸¸æ ‡ï¼Œä½¿ç”¨offsetåˆ†é¡µ
                if not cursor:
                    offset = (page - 1) * limit
                    base_query = base_query.offset(offset)

                # æ‰§è¡ŒæŸ¥è¯¢
                results = (
                    base_query.order_by(
                        WorkflowExecutionLog.created_at.asc(), WorkflowExecutionLog.id.asc()
                    )
                    .limit(limit + 1)
                    .all()
                )  # +1ç”¨äºæ£€æµ‹æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ

                # å¤„ç†ç»“æœ
                has_next = len(results) > limit
                if has_next:
                    results = results[:limit]

                logs = []
                last_item = None
                for log_entry in results:
                    log_dict = log_entry.to_dict()
                    log_dict["display_message"] = log_entry.display_message
                    logs.append(log_dict)
                    last_item = log_entry

                # ç”Ÿæˆä¸‹ä¸€é¡µæ¸¸æ ‡
                next_cursor = None
                if has_next and last_item:
                    cursor_data = {
                        "timestamp": last_item.created_at.isoformat(),
                        "id": last_item.id,
                    }
                    next_cursor = base64.b64encode(json.dumps(cursor_data).encode("utf-8")).decode(
                        "utf-8"
                    )

                # è®¡ç®—åˆ†é¡µä¿¡æ¯
                has_previous = page > 1 if not cursor else bool(cursor)

                return PaginationResult(
                    data=logs,
                    total_count=total_count,
                    page=page,
                    page_size=limit,
                    has_next=has_next,
                    has_previous=has_previous,
                    next_cursor=next_cursor,
                )

            finally:
                db.close()

        except Exception as e:
            self.logger.error(f"Failed to get paginated business logs from database: {e}")
            return PaginationResult(
                data=[],
                total_count=0,
                page=page,
                page_size=limit,
                has_next=False,
                has_previous=False,
            )

    async def _get_technical_logs_from_redis(
        self, execution_id: str, level: Optional[str], limit: int, offset: int
    ) -> List[Dict[str, Any]]:
        """ä»Redisè·å–æŠ€æœ¯æ—¥å¿—"""
        try:
            key = f"workflow_logs:technical:{execution_id}"
            raw_logs = self.redis_client.lrange(key, 0, -1)

            logs = []
            for raw_log in reversed(raw_logs):
                log = json.loads(raw_log)

                if level and log.get("level") != level:
                    continue

                logs.append(log)

            return logs[offset : offset + limit]

        except Exception as e:
            self.logger.error(f"Failed to get technical logs from Redis: {e}")
            return []

    async def _get_business_logs_from_redis_paginated(
        self, execution_id: str, min_priority: int, milestones_only: bool, limit: int, page: int
    ) -> PaginationResult:
        """ä»Redisè·å–ä¸šåŠ¡æ—¥å¿—ï¼ˆç®€å•åˆ†é¡µï¼‰"""
        try:
            key = f"workflow_logs:business:{execution_id}"
            raw_logs = self.redis_client.lrange(key, 0, -1)

            # è¿‡æ»¤æ—¥å¿—
            all_logs = []
            for raw_log in reversed(raw_logs):  # æŒ‰æ—¶é—´æ­£åº
                log = json.loads(raw_log)

                if log.get("display_priority", 5) < min_priority:
                    continue
                if milestones_only and not log.get("is_milestone", False):
                    continue

                all_logs.append(log)

            # åˆ†é¡µè®¡ç®—
            total_count = len(all_logs)
            offset = (page - 1) * limit
            logs = all_logs[offset : offset + limit]

            has_next = offset + limit < total_count
            has_previous = page > 1

            return PaginationResult(
                data=logs,
                total_count=total_count,
                page=page,
                page_size=limit,
                has_next=has_next,
                has_previous=has_previous,
            )

        except Exception as e:
            self.logger.error(f"Failed to get paginated business logs from Redis: {e}")
            return PaginationResult(
                data=[],
                total_count=0,
                page=page,
                page_size=limit,
                has_next=False,
                has_previous=False,
            )

    async def _get_technical_logs_from_redis_paginated(
        self, execution_id: str, level: Optional[str], limit: int, page: int
    ) -> PaginationResult:
        """ä»Redisè·å–æŠ€æœ¯æ—¥å¿—ï¼ˆç®€å•åˆ†é¡µï¼‰"""
        try:
            key = f"workflow_logs:technical:{execution_id}"
            raw_logs = self.redis_client.lrange(key, 0, -1)

            # è¿‡æ»¤æ—¥å¿—
            all_logs = []
            for raw_log in reversed(raw_logs):  # æŒ‰æ—¶é—´æ­£åº
                log = json.loads(raw_log)

                if level and log.get("level") != level:
                    continue

                all_logs.append(log)

            # åˆ†é¡µè®¡ç®—
            total_count = len(all_logs)
            offset = (page - 1) * limit
            logs = all_logs[offset : offset + limit]

            has_next = offset + limit < total_count
            has_previous = page > 1

            return PaginationResult(
                data=logs,
                total_count=total_count,
                page=page,
                page_size=limit,
                has_next=has_next,
                has_previous=has_previous,
            )

        except Exception as e:
            self.logger.error(f"Failed to get paginated technical logs from Redis: {e}")
            return PaginationResult(
                data=[],
                total_count=0,
                page=page,
                page_size=limit,
                has_next=False,
                has_previous=False,
            )

    async def _get_technical_logs_from_database(
        self, execution_id: str, level: Optional[str], limit: int, offset: int
    ) -> List[Dict[str, Any]]:
        """ä»æ•°æ®åº“è·å–æŠ€æœ¯æ—¥å¿—"""
        try:
            db = get_db_session()
            try:
                query = db.query(WorkflowExecutionLog).filter(
                    WorkflowExecutionLog.execution_id == execution_id,
                    WorkflowExecutionLog.log_category == LogCategoryEnum.TECHNICAL,
                )

                if level:
                    query = query.filter(WorkflowExecutionLog.level == LogLevelEnum(level))

                results = (
                    query.order_by(WorkflowExecutionLog.created_at.asc())
                    .offset(offset)
                    .limit(limit)
                    .all()
                )

                return [log_entry.to_dict() for log_entry in results]

            finally:
                db.close()

        except Exception as e:
            self.logger.error(f"Failed to get technical logs from database: {e}")
            return []

    async def _get_technical_logs_from_database_paginated(
        self,
        execution_id: str,
        level: Optional[str],
        limit: int,
        page: int,
        cursor: Optional[str] = None,
    ) -> PaginationResult:
        """ä»æ•°æ®åº“è·å–æŠ€æœ¯æ—¥å¿—ï¼ˆåˆ†é¡µï¼‰"""
        try:
            db = get_db_session()
            try:
                # æ„å»ºåŸºç¡€æŸ¥è¯¢
                base_query = db.query(WorkflowExecutionLog).filter(
                    WorkflowExecutionLog.execution_id == execution_id,
                    WorkflowExecutionLog.log_category == LogCategoryEnum.TECHNICAL,
                )

                if level:
                    base_query = base_query.filter(
                        WorkflowExecutionLog.level == LogLevelEnum(level)
                    )

                # è·å–æ€»æ•°é‡
                total_count = base_query.count()

                # å¤„ç†æ¸¸æ ‡åˆ†é¡µ
                if cursor:
                    try:
                        cursor_data = json.loads(base64.b64decode(cursor).decode("utf-8"))
                        cursor_timestamp = cursor_data.get("timestamp")
                        cursor_id = cursor_data.get("id")

                        if cursor_timestamp and cursor_id:
                            base_query = base_query.filter(
                                (WorkflowExecutionLog.created_at > cursor_timestamp)
                                | (
                                    (WorkflowExecutionLog.created_at == cursor_timestamp)
                                    & (WorkflowExecutionLog.id > cursor_id)
                                )
                            )
                    except Exception as e:
                        self.logger.warning(
                            f"Invalid cursor, falling back to offset pagination: {e}"
                        )
                        cursor = None

                # å¦‚æœæ²¡æœ‰æœ‰æ•ˆæ¸¸æ ‡ï¼Œä½¿ç”¨offsetåˆ†é¡µ
                if not cursor:
                    offset = (page - 1) * limit
                    base_query = base_query.offset(offset)

                # æ‰§è¡ŒæŸ¥è¯¢
                results = (
                    base_query.order_by(
                        WorkflowExecutionLog.created_at.asc(), WorkflowExecutionLog.id.asc()
                    )
                    .limit(limit + 1)
                    .all()
                )

                # å¤„ç†ç»“æœ
                has_next = len(results) > limit
                if has_next:
                    results = results[:limit]

                logs = [log_entry.to_dict() for log_entry in results]

                # ç”Ÿæˆä¸‹ä¸€é¡µæ¸¸æ ‡
                next_cursor = None
                if has_next and results:
                    last_item = results[-1]
                    cursor_data = {
                        "timestamp": last_item.created_at.isoformat(),
                        "id": last_item.id,
                    }
                    next_cursor = base64.b64encode(json.dumps(cursor_data).encode("utf-8")).decode(
                        "utf-8"
                    )

                has_previous = page > 1 if not cursor else bool(cursor)

                return PaginationResult(
                    data=logs,
                    total_count=total_count,
                    page=page,
                    page_size=limit,
                    has_next=has_next,
                    has_previous=has_previous,
                    next_cursor=next_cursor,
                )

            finally:
                db.close()

        except Exception as e:
            self.logger.error(f"Failed to get paginated technical logs from database: {e}")
            return PaginationResult(
                data=[],
                total_count=0,
                page=page,
                page_size=limit,
                has_next=False,
                has_previous=False,
            )

    def add_websocket_connection(self, execution_id: str, websocket: Any):
        """æ·»åŠ WebSocketè¿æ¥"""
        if execution_id not in self._websocket_connections:
            self._websocket_connections[execution_id] = []
        self._websocket_connections[execution_id].append(websocket)
        self.logger.info(f"Added WebSocket connection for execution {execution_id}")

    def remove_websocket_connection(self, execution_id: str, websocket: Any):
        """ç§»é™¤WebSocketè¿æ¥"""
        if execution_id in self._websocket_connections:
            try:
                self._websocket_connections[execution_id].remove(websocket)
                if not self._websocket_connections[execution_id]:
                    del self._websocket_connections[execution_id]
            except ValueError:
                pass
        self.logger.info(f"Removed WebSocket connection for execution {execution_id}")

    def clear_logs(self, execution_id: str):
        """æ¸…ç©ºæ—¥å¿—ç¼“å­˜"""
        if self.redis_client:
            self.redis_client.delete(f"workflow_logs:business:{execution_id}")
            self.redis_client.delete(f"workflow_logs:technical:{execution_id}")

        # æ¸…ç©ºå†…å­˜ç¼“å­˜
        keys_to_remove = [k for k in self._memory_cache.keys() if k.startswith(execution_id)]
        for key in keys_to_remove:
            del self._memory_cache[key]


# å…¨å±€æœåŠ¡å®ä¾‹
_unified_log_service: Optional[UnifiedExecutionLogService] = None


def get_unified_log_service() -> UnifiedExecutionLogService:
    """è·å–ç»Ÿä¸€æ—¥å¿—æœåŠ¡å•ä¾‹"""
    global _unified_log_service
    if _unified_log_service is None:
        _unified_log_service = UnifiedExecutionLogService()
    return _unified_log_service


# ä¾¿æ·å‡½æ•° - ä¸ç°æœ‰çš„BusinessLoggerå…¼å®¹
def create_legacy_compatible_logger(execution_id: str, workflow_name: str = "Unnamed Workflow"):
    """åˆ›å»ºä¸ç°æœ‰BusinessLoggerå…¼å®¹çš„æ—¥å¿—è®°å½•å™¨"""

    class LegacyCompatibleLogger:
        def __init__(self, execution_id: str, workflow_name: str):
            self.execution_id = execution_id
            self.workflow_name = workflow_name
            self.service = get_unified_log_service()

        async def workflow_started(self, total_steps: int, trigger_info: Optional[str] = None):
            trigger_msg = f" | è§¦å‘æ–¹å¼: {trigger_info}" if trigger_info else ""
            technical_msg = f"Starting workflow: {self.workflow_name} with {total_steps} steps"
            user_msg = f"ğŸš€ å¼€å§‹æ‰§è¡Œ: {self.workflow_name} | æ€»æ­¥éª¤: {total_steps}{trigger_msg}"

            await self.service.add_business_log(
                execution_id=self.execution_id,
                event_type="workflow_started",
                technical_message=technical_msg,
                user_friendly_message=user_msg,
                display_priority=DisplayPriorityEnum.HIGH.value if DisplayPriorityEnum else 7,
                is_milestone=True,
                total_steps=total_steps,
            )

        async def step_completed(
            self, step_name: str, duration_seconds: float, status: str = "SUCCESS"
        ):
            technical_msg = f"Step '{step_name}' completed with status: {status}"
            icon = "âœ…" if status == "SUCCESS" else "âŒ" if status == "ERROR" else "â¸ï¸"
            user_msg = f"{icon} {step_name} | çŠ¶æ€: {status} | è€—æ—¶: {duration_seconds:.1f}ç§’"

            await self.service.add_business_log(
                execution_id=self.execution_id,
                event_type="step_completed",
                technical_message=technical_msg,
                user_friendly_message=user_msg,
                display_priority=DisplayPriorityEnum.NORMAL.value if DisplayPriorityEnum else 5,
                duration_seconds=int(duration_seconds),
                node_name=step_name,
            )

    return LegacyCompatibleLogger(execution_id, workflow_name)
