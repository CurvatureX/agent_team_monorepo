"""
Workflowæ‰§è¡Œæ—¥å¿—æœåŠ¡
è´Ÿè´£å­˜å‚¨ã€ç¼“å­˜å’Œæ¨é€ç”¨æˆ·å‹å¥½çš„workflowæ‰§è¡Œæ—¥å¿—
"""

import asyncio
import json
import logging
import time
from collections import deque
from dataclasses import asdict, dataclass
from datetime import datetime, timedelta
from enum import Enum
from threading import Lock
from typing import Any, AsyncGenerator, Dict, List, Optional

# Import Unicode utilities for safe logging
try:
    from utils.unicode_utils import clean_unicode_data, safe_json_dumps, safe_json_loads

    UNICODE_UTILS_AVAILABLE = True
except ImportError:
    UNICODE_UTILS_AVAILABLE = False

try:
    import redis

    REDIS_AVAILABLE = True
except ImportError:
    REDIS_AVAILABLE = False

try:
    import sys
    from pathlib import Path

    # Import database connection from the main database module
    from database import Database
    from sqlalchemy.orm import Session

    # Import database models from shared package
    backend_dir = Path(__file__).parent.parent.parent.parent
    sys.path.insert(0, str(backend_dir))
    from shared.models.db_models import LogEventTypeEnum, LogLevelEnum, WorkflowExecutionLog

    DATABASE_AVAILABLE = True

except ImportError as e:
    DATABASE_AVAILABLE = False
    WorkflowExecutionLog = None
    LogLevelEnum = None
    LogEventTypeEnum = None


class LogEventType(str, Enum):
    """æ—¥å¿—äº‹ä»¶ç±»å‹"""

    WORKFLOW_STARTED = "workflow_started"
    WORKFLOW_COMPLETED = "workflow_completed"
    WORKFLOW_PROGRESS = "workflow_progress"
    STEP_STARTED = "step_started"
    STEP_INPUT = "step_input"
    STEP_OUTPUT = "step_output"
    STEP_COMPLETED = "step_completed"
    STEP_ERROR = "step_error"
    SEPARATOR = "separator"


@dataclass
class ExecutionLogEntry:
    """æ‰§è¡Œæ—¥å¿—æ¡ç›®"""

    execution_id: str
    event_type: LogEventType
    timestamp: str
    message: str
    data: Optional[Dict[str, Any]] = None
    level: str = "INFO"  # INFO, ERROR, DEBUG

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        result = asdict(self)
        result["timestamp"] = self.timestamp

        # Clean Unicode data before returning
        if UNICODE_UTILS_AVAILABLE:
            result = clean_unicode_data(result)

        return result


class ExecutionLogService:
    """
    å·¥ä½œæµæ‰§è¡Œæ—¥å¿—æœåŠ¡

    åŠŸèƒ½:
    1. å®æ—¶æ—¥å¿—å­˜å‚¨å’Œç¼“å­˜
    2. å†å²æ—¥å¿—æŸ¥è¯¢
    3. æµå¼æ—¥å¿—æ¨é€
    4. ç”¨æˆ·å‹å¥½çš„æ—¥å¿—æ ¼å¼åŒ–
    5. æ‰¹é‡å†™å…¥ä¼˜åŒ–ï¼ˆä»…ç”¨æˆ·å‹å¥½æ—¥å¿—å­˜å‚¨åˆ°Supabaseï¼‰
    """

    def __init__(self):
        self.logger = logging.getLogger(__name__)

        # Redisè¿æ¥ç”¨äºå®æ—¶ç¼“å­˜å’Œæ¨é€
        self.redis_client = None
        if REDIS_AVAILABLE:
            try:
                import os

                import redis

                # Use environment variable or fall back to localhost
                redis_url = os.getenv("REDIS_URL", "redis://localhost:6379/0")
                # Parse Redis URL and set db=1 for logs
                if "redis://" in redis_url:
                    # Extract base URL and set db=1
                    base_url = (
                        redis_url.split("/")[0] + "//" + redis_url.split("//")[1].split("/")[0]
                    )
                    redis_url = base_url + "/1"  # Use db=1 for logs

                self.redis_client = redis.from_url(redis_url, decode_responses=True)
                # æµ‹è¯•è¿æ¥
                self.redis_client.ping()
                self.logger.info("Redis connection established for log service")
            except Exception as e:
                self.logger.warning(f"Redis connection failed: {e}")
                self.redis_client = None

        # å†…å­˜ç¼“å­˜ä½œä¸ºRedisçš„å¤‡é€‰æ–¹æ¡ˆ
        self._memory_cache: Dict[str, List[ExecutionLogEntry]] = {}

        # WebSocketè¿æ¥ç®¡ç†
        self._websocket_connections: Dict[str, List[Any]] = {}

        # æ‰¹é‡å†™å…¥ç¼“å†²åŒº - ä»…å­˜å‚¨ç”¨æˆ·å‹å¥½æ—¥å¿—
        self._batch_buffer: deque = deque()
        self._buffer_lock = Lock()
        self._batch_writer_task = None
        self._shutdown = False

        # å¯åŠ¨æ‰¹é‡å†™å…¥ä»»åŠ¡
        self._start_batch_writer()

        # Note: Log cleanup (10-day TTL) is handled by database-level pg_cron job
        # See migration: 20250913000002_add_log_ttl_function.sql

    def _clean_log_entry(self, entry: ExecutionLogEntry) -> ExecutionLogEntry:
        """Clean log entry to prevent Unicode serialization issues."""
        if not UNICODE_UTILS_AVAILABLE:
            return entry

        try:
            # Clean the message
            cleaned_message = clean_unicode_data(entry.message)

            # Clean the data if present
            cleaned_data = None
            if entry.data:
                cleaned_data = clean_unicode_data(entry.data)

            # Create new entry with cleaned data
            return ExecutionLogEntry(
                execution_id=entry.execution_id,
                event_type=entry.event_type,
                timestamp=entry.timestamp,
                message=cleaned_message,
                data=cleaned_data,
                level=entry.level,
            )
        except Exception as e:
            self.logger.warning(f"Failed to clean log entry, using fallback: {e}")
            # Fallback: return entry with simplified data
            return ExecutionLogEntry(
                execution_id=entry.execution_id,
                event_type=entry.event_type,
                timestamp=entry.timestamp,
                message="[LOG_UNICODE_CLEANED]",
                data={"original_message": str(entry.message)[:100]},  # Truncate to prevent issues
                level=entry.level,
            )

    async def add_log_entry(self, entry: ExecutionLogEntry):
        """æ·»åŠ æ—¥å¿—æ¡ç›®"""
        try:
            # Clean the entry first to prevent Unicode issues
            entry = self._clean_log_entry(entry)
            self.logger.debug(f"ğŸ”¥ DEBUG: Adding log entry {entry.execution_id}: {entry.message}")

            # 1. å­˜å‚¨åˆ°Redisç¼“å­˜ï¼ˆæ‰€æœ‰æ—¥å¿—ï¼‰
            if self.redis_client:
                await self._store_to_redis(entry)
            else:
                # å¤‡é€‰ï¼šå­˜å‚¨åˆ°å†…å­˜ç¼“å­˜
                await self._store_to_memory(entry)

            # 2. æ¨é€åˆ°WebSocketè¿æ¥
            await self._push_to_websockets(entry)

            # 3. ä»…ç”¨æˆ·å‹å¥½æ—¥å¿—æ·»åŠ åˆ°æ‰¹é‡å†™å…¥ç¼“å†²åŒº
            is_user_friendly = self._is_user_friendly_log(entry)
            self.logger.debug(
                f"ğŸ”¥ DEBUG: Is user friendly: {is_user_friendly} for {entry.execution_id}"
            )

            if is_user_friendly:
                await self._add_to_batch_buffer(entry)
                self.logger.debug(f"ğŸ”¥ DEBUG: Added to batch buffer: {entry.execution_id}")

                # DIRECT DATABASE WRITE for debugging - bypass batch system
                try:
                    await self._direct_write_to_database(entry)
                    self.logger.debug(
                        f"ğŸ”¥ DEBUG: Direct database write successful for {entry.execution_id}"
                    )
                except Exception as direct_error:
                    self.logger.error(f"ğŸ”¥ Direct database write failed: {direct_error}")

        except Exception as e:
            self.logger.error(f"Failed to add log entry: {e}")

    async def _direct_write_to_database(self, entry: ExecutionLogEntry):
        """Direct database write for debugging - bypasses batch system"""
        if not DATABASE_AVAILABLE:
            return

        try:
            db = Database()
            node_info = self._extract_node_info_from_data(entry.data)

            log_record = {
                "execution_id": entry.execution_id,
                "log_category": "business",
                "event_type": entry.event_type.value,
                "level": entry.level.upper(),
                "message": entry.message,
                "data": entry.data or {},
                "node_id": node_info.get("node_id"),
                "node_name": node_info.get("node_name"),
                "node_type": node_info.get("node_type"),
                "step_number": node_info.get("step_number"),
                "total_steps": node_info.get("total_steps"),
                "duration_seconds": node_info.get("duration_seconds"),
                "user_friendly_message": entry.data.get("user_friendly_message")
                if entry.data
                else None,
                "display_priority": entry.data.get("display_priority", 5) if entry.data else 5,
                "is_milestone": entry.data.get("is_milestone", False) if entry.data else False,
            }

            # Clean the log record to ensure it's safe for database insertion
            if UNICODE_UTILS_AVAILABLE:
                log_record = clean_unicode_data(log_record)

            result = db.client.table("workflow_execution_logs").insert([log_record]).execute()
            # Clean the result before logging to prevent Unicode issues
            if UNICODE_UTILS_AVAILABLE:
                cleaned_result = clean_unicode_data(str(result)[:500])  # Truncate and clean
                self.logger.debug(f"ğŸ”¥ DEBUG: Direct insert result: {cleaned_result}")
            else:
                # Fallback: just log success without details
                self.logger.debug(f"ğŸ”¥ DEBUG: Direct insert completed successfully")

        except Exception as e:
            self.logger.error(f"Direct database write error: {e}")
            raise

    def _is_user_friendly_log(self, entry: ExecutionLogEntry) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºç”¨æˆ·å‹å¥½æ—¥å¿—ï¼ˆéœ€è¦å­˜å‚¨åˆ°Supabaseï¼‰"""
        # æ£€æŸ¥æ˜¯å¦æœ‰ç”¨æˆ·å‹å¥½çš„æ¶ˆæ¯æˆ–è€…æ˜¯é‡è¦çš„é‡Œç¨‹ç¢‘äº‹ä»¶
        if entry.data and entry.data.get("user_friendly_message"):
            return True

        # æ£€æŸ¥æ˜¯å¦ä¸ºé‡è¦çš„é‡Œç¨‹ç¢‘äº‹ä»¶
        milestone_events = {
            "workflow_started",
            "workflow_completed",
            "step_completed",
            "step_error",
        }

        if entry.event_type in milestone_events:
            return True

        # æ£€æŸ¥æ˜¯å¦ä¸ºé«˜ä¼˜å…ˆçº§æ—¥å¿—
        if entry.data and entry.data.get("display_priority", 5) >= 7:
            return True

        return False

    async def _add_to_batch_buffer(self, entry: ExecutionLogEntry):
        """æ·»åŠ ç”¨æˆ·å‹å¥½æ—¥å¿—åˆ°æ‰¹é‡å†™å…¥ç¼“å†²åŒº"""
        try:
            with self._buffer_lock:
                self._batch_buffer.append(entry)

            # å¦‚æœç¼“å†²åŒºè¿‡å¤§ï¼Œç«‹å³è§¦å‘å†™å…¥
            if len(self._batch_buffer) >= 50:  # è¾¾åˆ°50æ¡ç«‹å³å†™å…¥
                asyncio.create_task(self._flush_batch_buffer())

        except Exception as e:
            self.logger.error(f"Failed to add to batch buffer: {e}")

    def _start_batch_writer(self):
        """å¯åŠ¨æ‰¹é‡å†™å…¥åå°ä»»åŠ¡"""
        if not DATABASE_AVAILABLE:
            self.logger.debug("ğŸ”¥ DEBUG: DATABASE_AVAILABLE is False, batch writer not started")
            return

        self.logger.debug("ğŸ”¥ DEBUG: Starting batch writer initialization")

        async def batch_writer():
            self.logger.debug("ğŸ”¥ DEBUG: Batch writer started successfully")
            while not self._shutdown:
                try:
                    await asyncio.sleep(1.0)  # æ¯1ç§’æ‰§è¡Œä¸€æ¬¡
                    await self._flush_batch_buffer()
                except Exception as e:
                    self.logger.error(f"Batch writer error: {e}")

        # åœ¨äº‹ä»¶å¾ªç¯ä¸­å¯åŠ¨åå°ä»»åŠ¡
        try:
            # Try to get the running event loop (modern approach)
            loop = asyncio.get_running_loop()
            self._batch_writer_task = loop.create_task(batch_writer())
            self.logger.debug("ğŸ”¥ DEBUG: Batch writer task created successfully")
        except RuntimeError:
            # No running event loop, defer task creation
            self.logger.debug("ğŸ”¥ DEBUG: No running event loop, deferring batch writer start")
            self._batch_writer_task = None

    async def _flush_batch_buffer(self):
        """æ‰¹é‡å†™å…¥ç¼“å†²åŒºä¸­çš„æ—¥å¿—åˆ°æ•°æ®åº“"""
        if not DATABASE_AVAILABLE:
            self.logger.debug("ğŸ”¥ DEBUG: DATABASE_AVAILABLE is False, cannot flush")
            return

        # è·å–å¾…å†™å…¥çš„æ—¥å¿—æ¡ç›®
        entries_to_write = []
        with self._buffer_lock:
            if not self._batch_buffer:
                return  # ç¼“å†²åŒºä¸ºç©º

            # ä¸€æ¬¡æœ€å¤šå¤„ç†100æ¡æ—¥å¿—
            batch_size = min(len(self._batch_buffer), 100)
            self.logger.debug(f"ğŸ”¥ DEBUG: Flushing {batch_size} entries from buffer")
            for _ in range(batch_size):
                if self._batch_buffer:
                    entries_to_write.append(self._batch_buffer.popleft())

        if not entries_to_write:
            self.logger.debug("ğŸ”¥ DEBUG: No entries to write after dequeue")
            return

        try:
            # Use the Database class to get Supabase client
            db = Database()

            # Create log entries for Supabase
            log_records = []
            for entry in entries_to_write:
                # Extract node information
                node_info = self._extract_node_info_from_data(entry.data)

                # Extract user-friendly message and metadata
                user_friendly_message = None
                display_priority = 5
                is_milestone = False

                if entry.data:
                    user_friendly_message = entry.data.get("user_friendly_message")
                    display_priority = entry.data.get("display_priority", 5)
                    is_milestone = entry.data.get("is_milestone", False)

                # Create record for Supabase (note: created_at is auto-generated by database)
                log_record = {
                    "execution_id": entry.execution_id,
                    "log_category": "business",  # User-friendly logs are business category
                    "event_type": entry.event_type.value,
                    "level": entry.level.upper(),
                    "message": entry.message,
                    "data": entry.data or {},
                    "node_id": node_info.get("node_id"),
                    "node_name": node_info.get("node_name"),
                    "node_type": node_info.get("node_type"),
                    "step_number": node_info.get("step_number"),
                    "total_steps": node_info.get("total_steps"),
                    "duration_seconds": node_info.get("duration_seconds"),
                    "user_friendly_message": user_friendly_message,
                    "display_priority": display_priority,
                    "is_milestone": is_milestone
                    # Note: created_at timestamp is auto-generated by Supabase
                }

                # Clean the log record before adding to batch
                if UNICODE_UTILS_AVAILABLE:
                    log_record = clean_unicode_data(log_record)

                log_records.append(log_record)

            # Insert into Supabase
            if log_records:
                result = db.client.table("workflow_execution_logs").insert(log_records).execute()

                self.logger.info(
                    f"Successfully wrote {len(log_records)} user-friendly logs to database"
                )

        except Exception as e:
            self.logger.error(f"Failed to flush batch buffer to database: {e}")

            # å¤±è´¥çš„æ—¥å¿—é‡æ–°æ”¾å›ç¼“å†²åŒºå¤´éƒ¨
            with self._buffer_lock:
                for entry in reversed(entries_to_write):
                    self._batch_buffer.appendleft(entry)

    async def shutdown(self):
        """å…³é—­æœåŠ¡ï¼Œç¡®ä¿æ‰€æœ‰ç¼“å†²åŒºçš„æ—¥å¿—éƒ½è¢«å†™å…¥"""
        self._shutdown = True

        # ç­‰å¾…æ‰¹é‡å†™å…¥ä»»åŠ¡å®Œæˆ
        if self._batch_writer_task:
            await self._batch_writer_task

        # æœ€åä¸€æ¬¡åˆ·æ–°ç¼“å†²åŒº
        await self._flush_batch_buffer()

        self.logger.info("ExecutionLogService shutdown completed")

    async def _store_to_redis(self, entry: ExecutionLogEntry):
        """å­˜å‚¨åˆ°Redis"""
        try:
            key = f"workflow_logs:{entry.execution_id}"
            # Use safe JSON serialization to prevent Unicode issues
            if UNICODE_UTILS_AVAILABLE:
                value = safe_json_dumps(entry.to_dict())
            else:
                value = json.dumps(entry.to_dict(), ensure_ascii=True)

            # ä½¿ç”¨listå­˜å‚¨æ—¥å¿—æ¡ç›®ï¼ŒæŒ‰æ—¶é—´é¡ºåº
            self.redis_client.lpush(key, value)

            # è®¾ç½®è¿‡æœŸæ—¶é—´(24å°æ—¶)ï¼Œé¿å…Rediså†…å­˜æ— é™å¢é•¿
            self.redis_client.expire(key, 24 * 3600)

            # ä¹Ÿå­˜å‚¨åˆ°å…¨å±€æœ€è¿‘æ—¥å¿—åˆ—è¡¨(ç”¨äºç®¡ç†ç•Œé¢)
            self.redis_client.lpush("recent_workflow_logs", value)
            self.redis_client.ltrim("recent_workflow_logs", 0, 999)  # åªä¿ç•™æœ€è¿‘1000æ¡

        except Exception as e:
            self.logger.error(f"Failed to store to Redis: {e}")
            # å¤‡é€‰ï¼šå­˜å‚¨åˆ°å†…å­˜
            await self._store_to_memory(entry)

    async def _store_to_memory(self, entry: ExecutionLogEntry):
        """å­˜å‚¨åˆ°å†…å­˜ç¼“å­˜"""
        if entry.execution_id not in self._memory_cache:
            self._memory_cache[entry.execution_id] = []

        self._memory_cache[entry.execution_id].append(entry)

        # é™åˆ¶å†…å­˜ç¼“å­˜å¤§å°
        if len(self._memory_cache[entry.execution_id]) > 1000:
            self._memory_cache[entry.execution_id] = self._memory_cache[entry.execution_id][-500:]

    def _convert_to_db_event_type(self, event_type: LogEventType) -> LogEventTypeEnum:
        """è½¬æ¢äº‹ä»¶ç±»å‹åˆ°æ•°æ®åº“æšä¸¾"""
        # ç›´æ¥æ˜ å°„ï¼Œå› ä¸ºæšä¸¾å€¼ç›¸åŒ
        return LogEventTypeEnum(event_type.value)

    def _convert_to_db_level(self, level: str) -> LogLevelEnum:
        """è½¬æ¢æ—¥å¿—çº§åˆ«åˆ°æ•°æ®åº“æšä¸¾"""
        try:
            return LogLevelEnum(level.upper())
        except ValueError:
            return LogLevelEnum.INFO

    def _extract_node_info_from_data(self, data: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """ä»æ—¥å¿—æ•°æ®ä¸­æå–èŠ‚ç‚¹ä¿¡æ¯"""
        if not data:
            return {}

        node_info = {}

        # æå–èŠ‚ç‚¹ç›¸å…³ä¿¡æ¯
        if "node_id" in data:
            node_info["node_id"] = data["node_id"]
        if "node_name" in data or "step_name" in data:
            node_info["node_name"] = data.get("node_name") or data.get("step_name")
        if "node_type" in data:
            node_info["node_type"] = data["node_type"]

        # æå–æ­¥éª¤ä¿¡æ¯
        if "step_number" in data:
            node_info["step_number"] = data["step_number"]
        if "total_steps" in data:
            node_info["total_steps"] = data["total_steps"]

        # æå–æ€§èƒ½ä¿¡æ¯
        if "duration_seconds" in data:
            # ç¡®ä¿æ˜¯æ•´æ•°(æ•°æ®åº“å­—æ®µç±»å‹)
            try:
                node_info["duration_seconds"] = int(float(data["duration_seconds"]))
            except (ValueError, TypeError):
                pass

        return node_info

    async def _push_to_websockets(self, entry: ExecutionLogEntry):
        """æ¨é€åˆ°WebSocketè¿æ¥"""
        execution_id = entry.execution_id
        if execution_id in self._websocket_connections:
            # ç§»é™¤å·²æ–­å¼€çš„è¿æ¥
            active_connections = []
            for websocket in self._websocket_connections[execution_id]:
                try:
                    # Use safe JSON serialization for WebSocket messages
                    if UNICODE_UTILS_AVAILABLE:
                        message = safe_json_dumps(entry.to_dict())
                    else:
                        message = json.dumps(entry.to_dict(), ensure_ascii=True)
                    await websocket.send_text(message)
                    active_connections.append(websocket)
                except Exception:
                    # è¿æ¥å·²æ–­å¼€ï¼Œå¿½ç•¥
                    pass

            self._websocket_connections[execution_id] = active_connections
            if not active_connections:
                del self._websocket_connections[execution_id]

    async def get_logs(
        self, execution_id: str, limit: int = 100, offset: int = 0
    ) -> List[Dict[str, Any]]:
        """è·å–æ‰§è¡Œæ—¥å¿—(æ™®é€šæ¥å£)"""
        try:
            # 1. å…ˆå°è¯•ä»Redisè·å–
            if self.redis_client:
                logs = await self._get_logs_from_redis(execution_id, limit, offset)
                if logs:
                    return logs

            # 2. å¤‡é€‰ï¼šä»å†…å­˜ç¼“å­˜è·å–
            if execution_id in self._memory_cache:
                entries = self._memory_cache[execution_id]
                start_idx = offset
                end_idx = min(offset + limit, len(entries))
                return [entry.to_dict() for entry in entries[start_idx:end_idx]]

            # 3. æœ€åï¼šä»æ•°æ®åº“è·å–å†å²è®°å½•
            return await self._get_logs_from_database(execution_id, limit, offset)

        except Exception as e:
            self.logger.error(f"Failed to get logs: {e}")
            return []

    async def _get_logs_from_redis(
        self, execution_id: str, limit: int, offset: int
    ) -> List[Dict[str, Any]]:
        """ä»Redisè·å–æ—¥å¿—"""
        try:
            key = f"workflow_logs:{execution_id}"
            # Redisçš„listæ˜¯LIFOï¼Œéœ€è¦åå‘è·å–
            raw_logs = self.redis_client.lrange(key, offset, offset + limit - 1)
            logs = []
            for raw_log in reversed(raw_logs):  # åè½¬ä»¥è·å¾—æ­£ç¡®çš„æ—¶é—´é¡ºåº
                logs.append(json.loads(raw_log))
            return logs
        except Exception as e:
            self.logger.error(f"Failed to get logs from Redis: {e}")
            return []

    async def _get_logs_from_database(
        self, execution_id: str, limit: int, offset: int
    ) -> List[Dict[str, Any]]:
        """ä»æ•°æ®åº“è·å–å†å²æ—¥å¿—ï¼ˆä½¿ç”¨ Supabase Python å®¢æˆ·ç«¯ï¼‰"""
        try:
            db = Database()
            if not db.client:
                return []

            response = (
                db.client.table("workflow_execution_logs")
                .select("*")
                .eq("execution_id", execution_id)
                .order("created_at", desc=False)
                .range(offset, offset + limit - 1)
                .execute()
            )

            logs: List[Dict[str, Any]] = []
            for log_entry in response.data or []:
                log_dict = {
                    "execution_id": log_entry.get("execution_id"),
                    "event_type": log_entry.get("event_type"),
                    "timestamp": log_entry.get("created_at"),
                    "message": log_entry.get("message"),
                    "level": log_entry.get("level"),
                    "data": log_entry.get("data") or {},
                }

                # é™„åŠ èŠ‚ç‚¹ä¿¡æ¯
                for key in [
                    "node_id",
                    "node_name",
                    "node_type",
                    "step_number",
                    "total_steps",
                    "duration_seconds",
                ]:
                    if log_entry.get(key) is not None:
                        log_dict["data"][key] = log_entry.get(key)

                logs.append(log_dict)

            return logs

        except Exception as e:
            self.logger.error(f"Failed to get logs from database: {e}")
            return []

    def add_websocket_connection(self, execution_id: str, websocket: Any):
        """æ·»åŠ WebSocketè¿æ¥"""
        if execution_id not in self._websocket_connections:
            self._websocket_connections[execution_id] = []
        self._websocket_connections[execution_id].append(websocket)

        self.logger.info(f"Added WebSocket connection for execution {execution_id}")

    def remove_websocket_connection(self, execution_id: str, websocket: Any):
        """ç§»é™¤WebSocketè¿æ¥"""
        if execution_id in self._websocket_connections:
            try:
                self._websocket_connections[execution_id].remove(websocket)
                if not self._websocket_connections[execution_id]:
                    del self._websocket_connections[execution_id]
            except ValueError:
                pass

        self.logger.info(f"Removed WebSocket connection for execution {execution_id}")

    async def get_active_executions(self) -> List[Dict[str, Any]]:
        """è·å–å½“å‰æ´»è·ƒçš„æ‰§è¡Œåˆ—è¡¨"""
        active_executions = []

        if self.redis_client:
            # ä»Redisè·å–æ´»è·ƒæ‰§è¡Œ
            pattern = "workflow_logs:*"
            keys = self.redis_client.keys(pattern)

            for key in keys:
                execution_id = key.replace("workflow_logs:", "")
                # è·å–æœ€åä¸€æ¡æ—¥å¿—æ¥åˆ¤æ–­çŠ¶æ€
                latest_log_raw = self.redis_client.lindex(key, 0)
                if latest_log_raw:
                    latest_log = json.loads(latest_log_raw)
                    active_executions.append(
                        {
                            "execution_id": execution_id,
                            "last_activity": latest_log.get("timestamp"),
                            "status": self._infer_status_from_log(latest_log),
                        }
                    )

        return active_executions

    def _infer_status_from_log(self, log: Dict[str, Any]) -> str:
        """ä»æ—¥å¿—æ¨æ–­æ‰§è¡ŒçŠ¶æ€"""
        event_type = log.get("event_type")

        if event_type == LogEventType.WORKFLOW_COMPLETED:
            # æ£€æŸ¥æ˜¯å¦æˆåŠŸå®Œæˆ
            message = log.get("message", "")
            if "æˆåŠŸ" in message or "å®Œæˆ" in message:
                return "SUCCESS"
            elif "å¤±è´¥" in message or "é”™è¯¯" in message:
                return "ERROR"
            elif "æš‚åœ" in message:
                return "PAUSED"
        elif event_type == LogEventType.STEP_ERROR:
            return "ERROR"
        elif event_type in [LogEventType.STEP_STARTED, LogEventType.WORKFLOW_PROGRESS]:
            return "RUNNING"

        return "UNKNOWN"

    async def get_logs_with_filters(
        self,
        execution_id: str,
        limit: int = 100,
        offset: int = 0,
        level: Optional[str] = None,
        event_type: Optional[str] = None,
        start_time: Optional[datetime] = None,
        end_time: Optional[datetime] = None,
    ) -> Dict[str, Any]:
        """è·å–å¸¦è¿‡æ»¤æ¡ä»¶çš„æ‰§è¡Œæ—¥å¿—"""
        try:
            # ä¼˜å…ˆä»Redisè·å–(å®æ—¶æ•°æ®)
            if self.redis_client:
                logs = await self._get_filtered_logs_from_redis(
                    execution_id, limit, offset, level, event_type
                )
                if logs:
                    return {
                        "execution_id": execution_id,
                        "total_count": len(logs),
                        "logs": logs,
                        "pagination": {
                            "limit": limit,
                            "offset": offset,
                            "has_more": len(logs) >= limit,
                        },
                    }

            # ä»æ•°æ®åº“è·å–å†å²æ•°æ®
            if DATABASE_AVAILABLE:
                logs, total_count = await self._get_filtered_logs_from_database(
                    execution_id, limit, offset, level, event_type, start_time, end_time
                )
                return {
                    "execution_id": execution_id,
                    "total_count": total_count,
                    "logs": logs,
                    "pagination": {
                        "limit": limit,
                        "offset": offset,
                        "has_more": (offset + len(logs)) < total_count,
                    },
                }

            return {
                "execution_id": execution_id,
                "total_count": 0,
                "logs": [],
                "pagination": {"limit": limit, "offset": offset, "has_more": False},
            }

        except Exception as e:
            self.logger.error(f"Failed to get filtered logs: {e}")
            return {
                "execution_id": execution_id,
                "total_count": 0,
                "logs": [],
                "pagination": {"limit": limit, "offset": offset, "has_more": False},
            }

    async def _get_filtered_logs_from_redis(
        self,
        execution_id: str,
        limit: int,
        offset: int,
        level: Optional[str] = None,
        event_type: Optional[str] = None,
    ) -> List[Dict[str, Any]]:
        """ä»Redisè·å–è¿‡æ»¤åçš„æ—¥å¿—"""
        try:
            key = f"workflow_logs:{execution_id}"
            # è·å–æ‰€æœ‰æ—¥å¿—
            raw_logs = self.redis_client.lrange(key, 0, -1)
            logs = []

            for raw_log in reversed(raw_logs):  # æ—¶é—´é¡ºåº
                log = json.loads(raw_log)

                # åº”ç”¨è¿‡æ»¤æ¡ä»¶
                if level and log.get("level") != level:
                    continue
                if event_type and log.get("event_type") != event_type:
                    continue

                logs.append(log)

            # åº”ç”¨åˆ†é¡µ
            return logs[offset : offset + limit]

        except Exception as e:
            self.logger.error(f"Failed to get filtered logs from Redis: {e}")
            return []

    async def _get_filtered_logs_from_database(
        self,
        execution_id: str,
        limit: int,
        offset: int,
        level: Optional[str] = None,
        event_type: Optional[str] = None,
        start_time: Optional[datetime] = None,
        end_time: Optional[datetime] = None,
    ) -> tuple[List[Dict[str, Any]], int]:
        """ä»æ•°æ®åº“è·å–è¿‡æ»¤åçš„æ—¥å¿—ï¼ˆä½¿ç”¨ Supabase å®¢æˆ·ç«¯ï¼‰"""
        try:
            db = Database()
            if not db.client:
                return [], 0

            query = (
                db.client.table("workflow_execution_logs")
                .select("*", count="exact")
                .eq("execution_id", execution_id)
            )
            if level:
                query = query.eq("level", level.upper())
            if event_type:
                query = query.eq("event_type", event_type)
            if start_time:
                query = query.gte("created_at", start_time.isoformat())
            if end_time:
                query = query.lte("created_at", end_time.isoformat())

            resp = query.order("created_at", desc=False).range(offset, offset + limit - 1).execute()

            logs: List[Dict[str, Any]] = []
            for row in resp.data or []:
                logs.append(
                    {
                        "execution_id": row.get("execution_id"),
                        "event_type": row.get("event_type"),
                        "timestamp": row.get("created_at"),
                        "message": row.get("message"),
                        "level": row.get("level"),
                        "data": row.get("data") or {},
                    }
                )

            return logs, (resp.count or 0)

        except Exception as e:
            self.logger.error(f"Failed to get filtered logs from database: {e}")
            return [], 0

    async def get_execution_stats(self, execution_id: str) -> Dict[str, Any]:
        """è·å–æ‰§è¡Œç»Ÿè®¡ä¿¡æ¯"""
        try:
            # ä½¿ç”¨ Supabase è·å–ç»Ÿè®¡ä¿¡æ¯
            db = Database()
            if not db.client:
                return {}

            resp = (
                db.client.table("workflow_execution_logs")
                .select("*")
                .eq("execution_id", execution_id)
                .order("created_at", desc=False)
                .limit(1000)
                .execute()
            )

            rows = resp.data or []
            if not rows:
                return {}

            total_steps = 0
            completed_steps = 0
            failed_steps = 0
            total_duration = 0
            step_durations = []
            workflow_name = "æœªçŸ¥å·¥ä½œæµ"

            for row in rows:
                event_type = row.get("event_type")
                data = row.get("data") or {}
                if event_type == LogEventType.WORKFLOW_STARTED.value:
                    workflow_name = data.get("workflow_name", "æœªçŸ¥å·¥ä½œæµ")
                    total_steps = data.get("total_steps", 0) or 0
                elif event_type == LogEventType.STEP_COMPLETED.value:
                    status = data.get("status")
                    if status == "SUCCESS":
                        completed_steps += 1
                    else:
                        failed_steps += 1
                    duration = row.get("duration_seconds")
                    if duration:
                        step_durations.append(
                            {
                                "name": row.get("node_name") or f"æ­¥éª¤{row.get('step_number')}",
                                "duration": duration,
                            }
                        )
                        total_duration += duration

            slowest_step = (
                max(step_durations, key=lambda x: x["duration"]) if step_durations else None
            )
            average_step_time = total_duration / len(step_durations) if step_durations else 0

            return {
                "execution_id": execution_id,
                "workflow_name": workflow_name,
                "total_steps": total_steps,
                "completed_steps": completed_steps,
                "failed_steps": failed_steps,
                "total_duration": total_duration,
                "average_step_time": average_step_time,
                "slowest_step": slowest_step,
                "performance_metrics": {
                    "step_count": len(step_durations),
                    "success_rate": (completed_steps / (completed_steps + failed_steps))
                    if (completed_steps + failed_steps) > 0
                    else 0,
                },
            }

        except Exception as e:
            self.logger.error(f"Failed to get execution stats: {e}")
            return {}

    async def cleanup_old_logs(
        self, before_date: datetime, keep_recent: Optional[int] = None
    ) -> Dict[str, int]:
        """æ¸…ç†å†å²æ—¥å¿—"""
        try:
            db = Database()
            if not db.client:
                return {"deleted": 0}

            db.client.table("workflow_execution_logs").delete().lt(
                "created_at", before_date.isoformat()
            ).execute()

            return {"deleted": 0}

        except Exception as e:
            self.logger.error(f"Failed to cleanup old logs: {e}")
            return {"deleted": 0}

    async def get_log_stats(self) -> Dict[str, Any]:
        """è·å–æ—¥å¿—ç»Ÿè®¡ä¿¡æ¯"""
        try:
            stats = {}

            # Redisç»Ÿè®¡
            if self.redis_client:
                active_keys = self.redis_client.keys("workflow_logs:*")
                stats["active_executions"] = len(active_keys)

                # è®¡ç®—Redisç¼“å­˜ä½¿ç”¨æƒ…å†µ
                redis_info = self.redis_client.info("memory")
                stats["cache_size_mb"] = round(redis_info.get("used_memory", 0) / (1024 * 1024), 2)

            # æ•°æ®åº“ç»Ÿè®¡ï¼ˆSupabaseï¼‰
            try:
                db = Database()
                if db.client:
                    count_resp = (
                        db.client.table("workflow_execution_logs")
                        .select("id", count="exact")
                        .execute()
                    )
                    stats["total_log_entries"] = count_resp.count or 0
            except Exception:
                pass

            # å†…å­˜ç¼“å­˜ç»Ÿè®¡
            stats["memory_cache_executions"] = len(self._memory_cache)
            stats["active_websocket_connections"] = sum(
                len(connections) for connections in self._websocket_connections.values()
            )

            return stats

        except Exception as e:
            self.logger.error(f"Failed to get log stats: {e}")
            return {}


# å…¨å±€æœåŠ¡å®ä¾‹
_log_service: Optional[ExecutionLogService] = None


def get_execution_log_service() -> ExecutionLogService:
    """è·å–æ‰§è¡Œæ—¥å¿—æœåŠ¡å•ä¾‹"""
    global _log_service
    if _log_service is None:
        _log_service = ExecutionLogService()
    return _log_service
